{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ê¸°ë³¸ ë° ë°ì´í„° ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---\n",
    "import gc  # Garbage Collector: ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import glob  # íŒŒì¼ ê²½ë¡œëª…ì„ ì´ìš©í•´ ì›í•˜ëŠ” íŒŒì¼ë“¤ì„ ì°¾ì•„ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import os  # ìš´ì˜ì²´ì œì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (íŒŒì¼ ê²½ë¡œ ë“±)\n",
    "import json  # JSON í˜•ì‹ì˜ íŒŒì¼ì„ ì½ê³  ì“°ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pprint  # ë°ì´í„° êµ¬ì¡°ë¥¼ ì˜ˆì˜ê²Œ ì¶œë ¥í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "# --- ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ---\n",
    "import matplotlib.pyplot as plt  # ë°ì´í„° ì‹œê°í™”ë¥¼ ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "# --- ìˆ˜ì¹˜ ê³„ì‚° ë° ë°ì´í„° ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---\n",
    "import numpy as np  # ë‹¤ì°¨ì› ë°°ì—´ ë° ê³ ì„±ëŠ¥ ìˆ˜ì¹˜ ê³„ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pandas as pd  # ë°ì´í„°í”„ë ˆì„(í‘œ í˜•íƒœì˜ ë°ì´í„°)ì„ ë‹¤ë£¨ê¸° ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "# --- ë³‘ë ¬ ì²˜ë¦¬ ë° ìœ í‹¸ë¦¬í‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ---\n",
    "from joblib import Parallel, delayed  # ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from tqdm import tqdm  # ë°˜ë³µë¬¸ì˜ ì§„í–‰ ìƒíƒœë¥¼ ì‹œê°ì ì¸ ë§‰ëŒ€ë¡œ ë³´ì—¬ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from PIL import Image  # ì´ë¯¸ì§€ íŒŒì¼ì„ ì—´ê³  ë‹¤ë£¨ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Pillow)\n",
    "\n",
    "# --- Jupyter Notebook ì„¤ì • ---\n",
    "# matplotlibìœ¼ë¡œ ê·¸ë¦° ê·¸ë˜í”„ë¥¼ ë…¸íŠ¸ë¶ ì…€ ë°”ë¡œ ì•„ë˜ì— í‘œì‹œí•˜ë„ë¡ ì„¤ì •í•˜ëŠ” ë§¤ì§ ëª…ë ¹ì–´\n",
    "%matplotlib inline\n",
    "\n",
    "# --- Pandas ì¶œë ¥ ì˜µì…˜ ì„¤ì • ---\n",
    "# ë°ì´í„°í”„ë ˆì„ì„ ì¶œë ¥í•  ë•Œ ë³´ì—¬ì¤„ ìµœëŒ€ í–‰ê³¼ ì—´ì˜ ê°œìˆ˜ë¥¼ ì„¤ì •\n",
    "pd.options.display.max_rows = 128\n",
    "pd.options.display.max_columns = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa48d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ì‹œê°í™” ê·¸ë˜í”„ì˜ ê¸°ë³¸ í¬ê¸°ë¥¼ ì§€ì • ---\n",
    "plt.rcParams['figure.figsize'] = (12, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59822c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ë°ì´í„° ë””ë ‰í„°ë¦¬ì˜ ë‚´ìš©ì„ í™•ì¸ ---\n",
    "os.listdir('../input/test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c51b7f",
   "metadata": {},
   "source": [
    "- '../input/test/'\n",
    "  - Kaggle ë…¸íŠ¸ë¶ í™˜ê²½ì—ì„œ ëª¨ë“  ëŒ€íšŒ ë°ì´í„°ëŠ” ../input/ í´ë” ì•„ë˜ì— ìœ„ì¹˜í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ê²½ë¡œëŠ” ëŒ€íšŒì˜ test ë°ì´í„°ì…‹ í´ë”ë¥¼ ê°€ë¦¬í‚µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train/train.csv')\n",
    "test = pd.read_csv('../input/test/test.csv')\n",
    "sample_submission = pd.read_csv('../input/test/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedfe7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_breed = pd.read_csv('../input/breed_labels.csv')\n",
    "labels_state = pd.read_csv('../input/state_labels.csv')\n",
    "labels_color = pd.read_csv('../input/color_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_files = sorted(glob.glob('../input/train_images/*.jpg'))\n",
    "train_metadata_files = sorted(glob.glob('../input/train_metadata/*.json'))\n",
    "train_sentiment_files = sorted(glob.glob('../input/train_sentiment/*.json'))\n",
    "\n",
    "print('num of train images files: {}'.format(len(train_image_files)))\n",
    "print('num of train metadata files: {}'.format(len(train_metadata_files)))\n",
    "print('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n",
    "\n",
    "\n",
    "test_image_files = sorted(glob.glob('../input/test_images/*.jpg'))\n",
    "test_metadata_files = sorted(glob.glob('../input/test_metadata/*.json'))\n",
    "test_sentiment_files = sorted(glob.glob('../input/test_sentiment/*.json'))\n",
    "\n",
    "print('num of test images files: {}'.format(len(test_image_files)))\n",
    "print('num of test metadata files: {}'.format(len(test_metadata_files)))\n",
    "print('num of test sentiment files: {}'.format(len(test_sentiment_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b728e",
   "metadata": {},
   "source": [
    "CSV íŒŒì¼ì€ ê·¸ ìì²´ë¡œ **\"í•˜ë‚˜ì˜ ì™„ì„±ëœ í‘œ\"**ì´ê³ , ë¹„ì •í˜• ë°ì´í„°ëŠ” **\"ìˆ˜ë§ì€ ê°œë³„ ì¬ë£Œì˜ ëª©ë¡\"**ì´ê¸° ë•Œë¬¸ì— ë¹„ì •í˜•íŒŒì¼ë§Œ íŒŒì¼ì˜ ê°œìˆ˜ë¥¼ í™•ì¸\n",
    "\n",
    "## ë¹„ì •í˜• ë°ì´í„° íŒŒì¼ (.jpg, .json): \"ìˆ˜ë§ì€ ì¬ë£Œì˜ ëª©ë¡\"ì„ ìƒì„±\n",
    "- ì²˜ë¦¬ ë°©ì‹: glob.glob()ì€ ìˆ˜ì²œ ê°œì— ë‹¬í•˜ëŠ” ê°œë³„ íŒŒì¼ì˜ ë‚´ìš©ì„ ì½ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, íŒŒì¼ë“¤ì˜ ê²½ë¡œ(ìœ„ì¹˜)ë§Œ ì°¾ì•„ì„œ ë¦¬ìŠ¤íŠ¸(list)ë¡œ ë§Œë“­ë‹ˆë‹¤. ì•„ì§ì€ ì–´ë–¤ ì •ë³´ë„ ì²˜ë¦¬í•˜ì§€ ì•Šì€, ë§ ê·¸ëŒ€ë¡œ \"ì¬ë£Œ ëª©ë¡\"ë§Œ ë§Œë“  ìƒíƒœì…ë‹ˆë‹¤.\n",
    "\n",
    "- í™•ì¸ ëŒ€ìƒ: ì´ ë‹¨ê³„ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ëŠ” \"ê·¸ë˜ì„œ íŒŒì¼ì´ ì´ ëª‡ ê°œë‚˜ ìˆëŠ”ê°€?\" ì…ë‹ˆë‹¤. ì´ ê°œìˆ˜ë¥¼ í†µí•´ ë°ì´í„°ì˜ ì „ì²´ ê·œëª¨ë¥¼ íŒŒì•…í•˜ê³ , train.csv íŒŒì¼ì˜ í–‰ ê°œìˆ˜ì™€ ë¹„êµí•˜ë©° ë°ì´í„°ê°€ ëˆ„ë½ë˜ì§€ ì•Šì•˜ëŠ”ì§€ 1ì°¨ì ìœ¼ë¡œ ì ê²€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- ì ê²€ ë°©ë²•: ë”°ë¼ì„œ ë¦¬ìŠ¤íŠ¸ì— ë“¤ì–´ìˆëŠ” ê²½ë¡œê°€ ì´ ëª‡ ê°œì¸ì§€ len() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ê°œìˆ˜ë¥¼ ì„¸ì–´ ì¶œë ¥í•˜ëŠ” ê²ƒì´ ê°€ì¥ íš¨ê³¼ì ì¸ ì²« ë²ˆì§¸ ì ê²€ ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd94654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib ê·¸ë˜í”„ì˜ ì‹œê°ì  ìŠ¤íƒ€ì¼ì„ 'ggplot'ìœ¼ë¡œ ì„¤ì • (Rì–¸ì–´ì—ì„œ ìœ ë˜í•œ ì¸ê¸° ìŠ¤íƒ€ì¼)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# --- 1. ì´ë¯¸ì§€(Images) ë°ì´í„° ì»¤ë²„ë¦¬ì§€ í™•ì¸ ---\n",
    "# ì›ë³¸ train ë°ì´í„°ì—ì„œ 'PetID' ì»¬ëŸ¼ë§Œ ì¶”ì¶œí•˜ì—¬ ê¸°ì¤€ì´ ë  ID ëª©ë¡ ìƒì„±\n",
    "train_df_ids = train[['PetID']]\n",
    "# ì›ë³¸ train ë°ì´í„°ì˜ ì „ì²´ ë°˜ë ¤ë™ë¬¼ ìˆ˜ë¥¼ ì¶œë ¥\n",
    "print(train_df_ids.shape)\n",
    "\n",
    "# ì´ì „ì— ìˆ˜ì§‘í•œ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "train_df_imgs = pd.DataFrame(train_image_files)\n",
    "train_df_imgs.columns = ['image_filename'] # ì»¬ëŸ¼ ì´ë¦„ ì§€ì •\n",
    "# íŒŒì¼ëª…ì—ì„œ PetIDë¥¼ ì¶”ì¶œ. ì˜ˆ: '../.../a1b2c3d4-1.jpg' -> 'a1b2c3d4'\n",
    "train_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "# ì¶”ì¶œí•œ PetIDë¥¼ 'PetID'ë¼ëŠ” ìƒˆ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€\n",
    "train_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\n",
    "# ì´ë¯¸ì§€ê°€ ìˆëŠ” ë°˜ë ¤ë™ë¬¼ì˜ ê³ ìœ í•œ ID ê°œìˆ˜ë¥¼ ì¶œë ¥\n",
    "print(len(train_imgs_pets.unique()))\n",
    "\n",
    "# ì´ë¯¸ì§€ íŒŒì¼ì˜ PetIDì™€ ì›ë³¸ train ë°ì´í„°ì˜ PetID ì‚¬ì´ì˜ êµì§‘í•©(ê³µí†µ ID)ì„ êµ¬í•¨\n",
    "pets_with_images = len(np.intersect1d(train_imgs_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "# ì›ë³¸ ë°ì´í„°ì˜ ëª¨ë“  ë°˜ë ¤ë™ë¬¼ ì¤‘, ì´ë¯¸ì§€ê°€ ìˆëŠ” ë™ë¬¼ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ì—¬ ì¶œë ¥\n",
    "print('fraction of pets with images: {:.3f}'.format(pets_with_images / train_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# --- 2. ë©”íƒ€ë°ì´í„°(Metadata) ì»¤ë²„ë¦¬ì§€ í™•ì¸ ---\n",
    "# (ìœ„ ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬ì™€ ê³¼ì •ì´ ê±°ì˜ ë™ì¼)\n",
    "train_df_ids = train[['PetID']]\n",
    "train_df_metadata = pd.DataFrame(train_metadata_files)\n",
    "train_df_metadata.columns = ['metadata_filename']\n",
    "# íŒŒì¼ëª…ì—ì„œ PetIDë¥¼ ì¶”ì¶œ. ì˜ˆ: '../.../a1b2c3d4-1.json' -> 'a1b2c3d4'\n",
    "train_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "train_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\n",
    "print(len(train_metadata_pets.unique()))\n",
    "\n",
    "pets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "# ì›ë³¸ ë°ì´í„°ì˜ ëª¨ë“  ë°˜ë ¤ë™ë¬¼ ì¤‘, ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ” ë™ë¬¼ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ì—¬ ì¶œë ¥\n",
    "print('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / train_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# --- 3. ê°ì„± ë¶„ì„(Sentiment) ë°ì´í„° ì»¤ë²„ë¦¬ì§€ í™•ì¸ ---\n",
    "# (ìœ„ ë°ì´í„° ì²˜ë¦¬ì™€ ê³¼ì •ì´ ê±°ì˜ ë™ì¼í•˜ë‚˜, PetID ì¶”ì¶œ ë°©ì‹ì— ì°¨ì´ê°€ ìˆìŒ)\n",
    "train_df_ids = train[['PetID']]\n",
    "train_df_sentiment = pd.DataFrame(train_sentiment_files)\n",
    "train_df_sentiment.columns = ['sentiment_filename']\n",
    "# íŒŒì¼ëª…ì—ì„œ PetIDë¥¼ ì¶”ì¶œ. ì˜ˆ: '../.../a1b2c3d4.json' -> 'a1b2c3d4'\n",
    "# ì—¬ê¸°ì„œëŠ” íŒŒì¼ëª…ì— '-'ê°€ ì—†ìœ¼ë¯€ë¡œ '.'ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬\n",
    "train_sentiment_pets = train_df_sentiment['filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "train_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\n",
    "print(len(train_sentiment_pets.unique()))\n",
    "\n",
    "pets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "# ì›ë³¸ ë°ì´í„°ì˜ ëª¨ë“  ë°˜ë ¤ë™ë¬¼ ì¤‘, ê°ì„± ë¶„ì„ ë°ì´í„°ê°€ ìˆëŠ” ë™ë¬¼ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ì—¬ ì¶œë ¥\n",
    "print('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / train_df_ids.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081b887",
   "metadata": {},
   "source": [
    "ì•ì—ì„œ ë³´ì•˜ë“¯ì´, ë¹„ì •í˜• ë°ì´í„° íŒŒì¼ì˜ ë‚´ë¶€ì—ëŠ” ì–´ë–¤ ë‚´ìš©ì´ ìˆëŠ”ì§€ ë°”ë¡œ íŒŒì•…í•  ìˆ˜ ì—†ë‹¤. <br>\n",
    "ë”°ë¼ì„œ \"ì „ì²´ ë™ë¬¼ ì¤‘ ëª‡ í¼ì„¼íŠ¸ê°€ ê° ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆëŠ”ê°€?\" ì¦‰, ë°ì´í„°ì˜ ì´ìš© ê°€ëŠ¥ì„±ì„ íŒŒì•…í•œë‹¤.<br>\n",
    "\n",
    "ì´ ê³¼ì •ì„ í†µí•´ ê° ë¹„ì •í˜• ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ìœ ìš©í•œì§€, ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš° ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€(ì˜ˆ: íŠ¹ì • ë°ì´í„°ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê±°ë‚˜, ì—†ëŠ” ê°’ì„ ì±„ì›Œ ë„£ëŠ” ë“±)ì— ëŒ€í•œ ë¶„ì„ ì „ëµì„ ì„¸ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸(Test Set)ì˜ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ í™•ì¸ ---\n",
    "# ì•„ë˜ 3ê°œì˜ ì„¹ì…˜(Images, Metadata, Sentiment)ì€\n",
    "# ì´ì „ì— Train ì„¸íŠ¸ì— ëŒ€í•´ ìˆ˜í–‰í–ˆë˜ ê²ƒê³¼ ì™„ì „íˆ ë™ì¼í•œ ê³¼ì •ì…ë‹ˆë‹¤.\n",
    "# ë‹¨ì§€ ëŒ€ìƒì´ test ë°ì´í„°ë¼ëŠ” ì ë§Œ ë‹¤ë¦…ë‹ˆë‹¤.\n",
    "\n",
    "# Images:\n",
    "test_df_ids = test[['PetID']]\n",
    "print(test_df_ids.shape)\n",
    "\n",
    "test_df_imgs = pd.DataFrame(test_image_files)\n",
    "test_df_imgs.columns = ['image_filename']\n",
    "test_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "test_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n",
    "print(len(test_imgs_pets.unique()))\n",
    "\n",
    "pets_with_images = len(np.intersect1d(test_imgs_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ëª¨ë“  ë°˜ë ¤ë™ë¬¼ ì¤‘, ì´ë¯¸ì§€ê°€ ìˆëŠ” ë™ë¬¼ì˜ ë¹„ìœ¨ì„ ê³„ì‚°\n",
    "print('fraction of pets with images: {:.3f}'.format(pets_with_images / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# Metadata:\n",
    "test_df_ids = test[['PetID']]\n",
    "test_df_metadata = pd.DataFrame(test_metadata_files)\n",
    "test_df_metadata.columns = ['metadata_filename']\n",
    "test_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "test_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\n",
    "print(len(test_metadata_pets.unique()))\n",
    "\n",
    "pets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ëª¨ë“  ë°˜ë ¤ë™ë¬¼ ì¤‘, ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ” ë™ë¬¼ì˜ ë¹„ìœ¨ì„ ê³„ì‚°\n",
    "print('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# Sentiment:\n",
    "test_df_ids = test[['PetID']]\n",
    "test_df_sentiment = pd.DataFrame(test_sentiment_files)\n",
    "test_df_sentiment.columns = ['sentiment_filename']\n",
    "test_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "test_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\n",
    "print(len(test_sentiment_pets.unique()))\n",
    "\n",
    "pets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ëª¨ë“  ë°˜ë ¤ë™ë¬¼ ì¤‘, ê°ì„± ë¶„ì„ ë°ì´í„°ê°€ ìˆëŠ” ë™ë¬¼ì˜ ë¹„ìœ¨ì„ ê³„ì‚°\n",
    "print('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# --- 2. ë°ì´í„° ì¼ê´€ì„± ìµœì¢… í™•ì¸ ---\n",
    "# ì´ë¯¸ì§€ íŒŒì¼ì—ì„œ ì¶”ì¶œí•œ PetID ëª©ë¡ê³¼ ë©”íƒ€ë°ì´í„° íŒŒì¼ì—ì„œ ì¶”ì¶œí•œ PetID ëª©ë¡ì´\n",
    "# ìˆœì„œê¹Œì§€ ì™„ì „íˆ ë™ì¼í•œì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "print('images and metadata distributions the same? {}'.format(\n",
    "    np.all(test_metadata_pets == test_imgs_pets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4515f",
   "metadata": {},
   "source": [
    "ìœ„ì—ì„œ ìˆ˜í–‰í•œ ì‘ì—…ì´ train ì…‹ì— ëŒ€í•œ ì‘ì—…ì´ì—ˆë‹¤ë©´, ì´ ì‘ì—…ì€ test ì…‹ì— ëŒ€í•œ ì‘ì—…ë‹ˆë‹¤.\n",
    "\n",
    "ë§ˆì§€ë§‰ í•œ ì¤„ì˜ ì½”ë“œëŠ” ë°ì´í„°ì˜ ì¼ê´€ì„±ì„ ê²€ì‚¬í•˜ëŠ” ì½”ë“œì´ë‹¤.\n",
    "ì½”ë“œì˜ ì˜ë¯¸ëŠ”\n",
    "```\n",
    "test_metadata_pets == test_imgs_pets: metadata íŒŒì¼ì—ì„œ ì¶”ì¶œí•œ PetID ëª©ë¡ê³¼ image íŒŒì¼ì—ì„œ ì¶”ì¶œí•œ PetID ëª©ë¡ì„ í•­ëª©ë³„ë¡œ í•˜ë‚˜í•˜ë‚˜ ë¹„êµí•©ë‹ˆë‹¤. ë‘ ëª©ë¡ì˜ ê¸¸ì´ê°€ ê°™ê³ , ê°™ì€ ìœ„ì¹˜ì— ìˆëŠ” PetIDê°€ ì„œë¡œ ë™ì¼í•˜ë©´ True, í•˜ë‚˜ë¼ë„ ë‹¤ë¥´ë©´ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "np.all(): ìœ„ ë¹„êµ ê²°ê³¼ê°€ ëª¨ë‘ Trueì¼ ë•Œë§Œ ìµœì¢…ì ìœ¼ë¡œ Trueë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "```\n",
    "ë§Œì•½ ì´ ê²°ê³¼ê°€ Trueë¼ë©´, ì´ëŠ” **\"ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ” ëª¨ë“  ë™ë¬¼ì€ ì´ë¯¸ì§€ë„ ê°€ì§€ê³  ìˆìœ¼ë©°, ê·¸ ìˆœì„œê¹Œì§€ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•œë‹¤\"**ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„°ê°€ ë§¤ìš° ê¹¨ë—í•˜ê³  ì •ë¦¬ê°€ ì˜ ë˜ì–´ìˆë‹¤ëŠ” ê¸ì •ì ì¸ ì‹ í˜¸   \n",
    "\n",
    "\n",
    "ì‹¤ì œ Outputì€ TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹„ì •í˜• ë°ì´í„°(JSON, ì´ë¯¸ì§€)ë¥¼ ì²˜ë¦¬í•˜ê³  ë¶„ì„ ê°€ëŠ¥í•œ íŠ¹ì§•(feature)ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ í´ë˜ìŠ¤\n",
    "class PetFinderParser(object):\n",
    "    \n",
    "    # í´ë˜ìŠ¤ê°€ ìƒì„±ë  ë•Œ ì´ˆê¸° ì„¤ì •ì„ ë‹´ë‹¹í•˜ëŠ” ìƒì„±ì í•¨ìˆ˜\n",
    "    def __init__(self, debug=False):\n",
    "        \n",
    "        self.debug = debug  # ë””ë²„ê·¸ ëª¨ë“œ ì—¬ë¶€ (í˜„ì¬ ì½”ë“œì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)\n",
    "        self.sentence_sep = ' '  # ë¬¸ì¥ì´ë‚˜ ë‹¨ì–´ë¥¼ í•©ì¹  ë•Œ ì‚¬ìš©í•  êµ¬ë¶„ì (ê³µë°±)\n",
    "        \n",
    "        # ì£¼ì„: ë©”ì¸ ë°ì´í„°í”„ë ˆì„ì— ì´ë¯¸ 'description'ì´ ìˆìœ¼ë¯€ë¡œ, sentiment íŒŒì¼ì—ì„œ\n",
    "        # í…ìŠ¤íŠ¸ë¥¼ ì¤‘ë³µìœ¼ë¡œ ì¶”ì¶œí•  í•„ìš”ê°€ ì—†ë‹¤ëŠ” ì˜ë¯¸.\n",
    "        self.extract_sentiment_text = False # ê°ì„± ë¶„ì„ íŒŒì¼ì—ì„œ ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí• ì§€ ì—¬ë¶€\n",
    "        \n",
    "        \n",
    "    # --- íŒŒì¼ ë¡œë“œ í•¨ìˆ˜ë“¤ ---\n",
    "    \n",
    "    def open_metadata_file(self, filename):\n",
    "        \"\"\"\n",
    "        ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ì„ ì—´ê³  ë‚´ìš©ì„ ì½ì–´ì˜µë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as f: # íŒŒì¼ì„ ì½ê¸° ëª¨ë“œ('r')ë¡œ ì—´ê¸°\n",
    "            metadata_file = json.load(f) # JSON íŒŒì¼ì˜ ë‚´ìš©ì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "        return metadata_file\n",
    "            \n",
    "    def open_sentiment_file(self, filename):\n",
    "        \"\"\"\n",
    "        ê°ì„± ë¶„ì„ JSON íŒŒì¼ì„ ì—´ê³  ë‚´ìš©ì„ ì½ì–´ì˜µë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as f: # íŒŒì¼ì„ ì½ê¸° ëª¨ë“œ('r')ë¡œ ì—´ê¸°\n",
    "            sentiment_file = json.load(f) # JSON íŒŒì¼ì˜ ë‚´ìš©ì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "        return sentiment_file\n",
    "            \n",
    "    def open_image_file(self, filename):\n",
    "        \"\"\"\n",
    "        ì´ë¯¸ì§€ íŒŒì¼ì„ ì—´ê³  numpy ë°°ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. (ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)\n",
    "        \"\"\"\n",
    "        image = np.asarray(Image.open(filename)) # ì´ë¯¸ì§€ë¥¼ ìˆ«ì ë°°ì—´ë¡œ ë³€í™˜\n",
    "        return image\n",
    "        \n",
    "    # --- íŒŒì¼ ë‚´ìš© íŒŒì‹±(Parsing) ë° íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ë“¤ ---\n",
    "        \n",
    "    def parse_sentiment_file(self, file):\n",
    "        \"\"\"\n",
    "        ê°ì„± ë¶„ì„ íŒŒì¼(ë”•ì…”ë„ˆë¦¬)ì„ ì…ë ¥ë°›ì•„, ê°ì„± íŠ¹ì§•ì´ ë‹´ê¸´ ë°ì´í„°í”„ë ˆì„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        \n",
    "        # ë¬¸ì„œ ì „ì²´ì˜ ê°ì„± ì ìˆ˜(magnitude, score)ë¥¼ ì¶”ì¶œ\n",
    "        file_sentiment = file['documentSentiment']\n",
    "        # í…ìŠ¤íŠ¸ì—ì„œ ì¸ì‹ëœ ì£¼ìš” ê°œì²´(ê³ ì–‘ì´, ì¥ë‚œê° ë“±)ì˜ ì´ë¦„ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ\n",
    "        file_entities = [x['name'] for x in file['entities']]\n",
    "        # ê°œì²´ ì´ë¦„ë“¤ì„ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨\n",
    "        file_entities = self.sentence_sep.join(file_entities)\n",
    "\n",
    "        # self.extract_sentiment_textê°€ Trueì¼ ë•Œë§Œ ì‹¤í–‰ë˜ëŠ” ë¸”ë¡ (í˜„ì¬ëŠ” False)\n",
    "        if self.extract_sentiment_text:\n",
    "            # ëª¨ë“  ë¬¸ì¥ì˜ ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨\n",
    "            file_sentences_text = [x['text']['content'] for x in file['sentences']]\n",
    "            file_sentences_text = self.sentence_sep.join(file_sentences_text)\n",
    "            # ëª¨ë“  ë¬¸ì¥ ê°ê°ì˜ ê°ì„± ì ìˆ˜ë¥¼ ì¶”ì¶œ\n",
    "            file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n",
    "            \n",
    "            # ë¬¸ì¥ë³„ ê°ì„± ì ìˆ˜ë“¤ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“¤ê³ , ê° ì ìˆ˜(magnitude, score)ì˜ í•©ê³„ë¥¼ êµ¬í•¨\n",
    "            file_sentences_sentiment = pd.DataFrame.from_dict(\n",
    "                file_sentences_sentiment, orient='columns').sum()\n",
    "            # ê³„ì‚°ëœ ì ìˆ˜ ì»¬ëŸ¼ëª…ì— 'document_' ì ‘ë‘ì‚¬ë¥¼ ì¶”ê°€í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "            file_sentences_sentiment = file_sentences_sentiment.add_prefix('document_').to_dict()\n",
    "            \n",
    "            # ë¬¸ì„œ ì „ì²´ ê°ì„± ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ì— ë¬¸ì¥ë³„ í•©ì‚° ì ìˆ˜ë¥¼ ì¶”ê°€\n",
    "            file_sentiment.update(file_sentences_sentiment)\n",
    "        \n",
    "        # ìµœì¢… ê°ì„± ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n",
    "        if self.extract_sentiment_text:\n",
    "            # í…ìŠ¤íŠ¸ ì¶”ì¶œì´ Trueì˜€ë‹¤ë©´, 'text' ì»¬ëŸ¼ì„ ì¶”ê°€\n",
    "            df_sentiment['text'] = file_sentences_text\n",
    "            \n",
    "        # ê°œì²´(entities) ì •ë³´ë¥¼ 'entities' ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€\n",
    "        df_sentiment['entities'] = file_entities\n",
    "        # ëª¨ë“  ì»¬ëŸ¼ëª…ì— 'sentiment_' ì ‘ë‘ì‚¬ë¥¼ ë¶™ì—¬ ë‹¤ë¥¸ ë°ì´í„°ì™€ êµ¬ë¶„\n",
    "        df_sentiment = df_sentiment.add_prefix('sentiment_')\n",
    "        \n",
    "        return df_sentiment\n",
    "    \n",
    "    def parse_metadata_file(self, file):\n",
    "        \"\"\"\n",
    "        ë©”íƒ€ë°ì´í„° íŒŒì¼(ë”•ì…”ë„ˆë¦¬)ì„ ì…ë ¥ë°›ì•„, ì´ë¯¸ì§€ íŠ¹ì§•ì´ ë‹´ê¸´ ë°ì´í„°í”„ë ˆì„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        \n",
    "        # íŒŒì¼ì— ì–´ë–¤ ìµœìƒìœ„ í‚¤ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "        file_keys = list(file.keys())\n",
    "        \n",
    "        # 'labelAnnotations' í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì´ë¯¸ì§€ ë¼ë²¨ ì •ë³´)\n",
    "        if 'labelAnnotations' in file_keys:\n",
    "            # ì¸ì‹ëœ ë¼ë²¨ ì¤‘ ìƒìœ„ 30%ë§Œ ì‚¬ìš© (ì •ë³´ê°€ ë„ˆë¬´ ë§ì€ ê²ƒì„ ë°©ì§€)\n",
    "            file_annots = file['labelAnnotations'][:int(len(file['labelAnnotations']) * 0.3)]\n",
    "            # ìƒìœ„ ë¼ë²¨ë“¤ì˜ ì‹ ë¢°ë„ ì ìˆ˜(score)ì˜ í‰ê· ì„ ê³„ì‚°\n",
    "            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n",
    "            # ìƒìœ„ ë¼ë²¨ë“¤ì˜ ì„¤ëª…(description)ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
    "            file_top_desc = [x['description'] for x in file_annots]\n",
    "        else:\n",
    "            # ë¼ë²¨ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°, ì ìˆ˜ëŠ” NaN(ê²°ì¸¡ì¹˜)ìœ¼ë¡œ, ì„¤ëª…ì€ ë¹ˆì¹¸ìœ¼ë¡œ ì²˜ë¦¬\n",
    "            file_top_score = np.nan\n",
    "            file_top_desc = ['']\n",
    "        \n",
    "        # ì´ë¯¸ì§€ì˜ ì§€ë°°ì ì¸ ìƒ‰ìƒ ì •ë³´ì™€ êµ¬ë„(crop) ì •ë³´ë¥¼ ì¶”ì¶œ\n",
    "        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "        file_crops = file['cropHintsAnnotation']['cropHints']\n",
    "\n",
    "        # ê° ìƒ‰ìƒì˜ ì‹ ë¢°ë„ ì ìˆ˜ì™€ í”½ì…€ ë¹„ìœ¨ì˜ í‰ê· ì„ ê³„ì‚°\n",
    "        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "\n",
    "        # êµ¬ë„ ì¶”ì²œ ì ìˆ˜(confidence)ì˜ í‰ê· ì„ ê³„ì‚°\n",
    "        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n",
    "        \n",
    "        # êµ¬ë„ ì •ë³´ì— 'importanceFraction' í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì—†ëŠ” ê²½ìš°ë„ ìˆìŒ)\n",
    "        if 'importanceFraction' in file_crops[0].keys():\n",
    "            # ì¤‘ìš”ë„(importance) ì ìˆ˜ì˜ í‰ê· ì„ ê³„ì‚°\n",
    "            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n",
    "        else:\n",
    "            # ì—†ëŠ” ê²½ìš°, NaNìœ¼ë¡œ ì²˜ë¦¬\n",
    "            file_crop_importance = np.nan\n",
    "\n",
    "        # ìœ„ì—ì„œ ì¶”ì¶œí•˜ê³  ê³„ì‚°í•œ íŠ¹ì§•ë“¤ì„ í•˜ë‚˜ì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ì •ë¦¬\n",
    "        df_metadata = {\n",
    "            'annots_score': file_top_score,\n",
    "            'color_score': file_color_score,\n",
    "            'color_pixelfrac': file_color_pixelfrac,\n",
    "            'crop_conf': file_crop_conf,\n",
    "            'crop_importance': file_crop_importance,\n",
    "            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n",
    "        }\n",
    "        \n",
    "        # ì •ë¦¬ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n",
    "        # ëª¨ë“  ì»¬ëŸ¼ëª…ì— 'metadata_' ì ‘ë‘ì‚¬ë¥¼ ë¶™ì—¬ ë‹¤ë¥¸ ë°ì´í„°ì™€ êµ¬ë¶„\n",
    "        df_metadata = df_metadata.add_prefix('metadata_')\n",
    "        \n",
    "        return df_metadata\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cb15c5",
   "metadata": {},
   "source": [
    "ğŸ” íŒŒì‹±(Parsing): ë¬¸ìì—´ì´ë‚˜ íŒŒì¼, JSON ë“±ì˜ ë³µì¡í•œ ë°ì´í„°ë¥¼ ì¼ì •í•œ ê·œì¹™ìœ¼ë¡œ ë¶„ì„í•˜ê³ , êµ¬ì¡°í™”í•˜ëŠ” ê³¼ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa604d",
   "metadata": {},
   "source": [
    "1. ë³µì¡í•œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ JSON íŒŒì¼ ë‚´ìš©ì„ ì…ë ¥ë°›ì•„, ê·¸ ì•ˆì—ì„œ ì˜ë¯¸ ìˆëŠ” ì •ë³´(ê°ì„± ì ìˆ˜, ì´ë¯¸ì§€ ë¼ë²¨, ìƒ‰ìƒ ì •ë³´ ë“±)ë§Œ ê³¨ë¼ë‚´ê³  ê³„ì‚°(í‰ê·  ë“±)í•˜ì—¬, ìµœì¢…ì ìœ¼ë¡œëŠ” ë¶„ì„í•˜ê¸° ì‰¬ìš´ í•œ ì¤„ì§œë¦¬ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“¤ì–´ ë°˜í™˜   \n",
    "íŒŒì´ì¬ ì½”ë“œê°€ ì§ì ‘ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆê³ , ê·¸ëƒ¥ json íŒŒì¼ì„ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€ì— ëŒ€í•œ ê·œì¹™ì„ ì •í•´ì£¼ëŠ” ì½”ë“œì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc17e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í—¬í¼(Helper) í•¨ìˆ˜ ---\n",
    "\n",
    "# PetID í•˜ë‚˜ë¥¼ ì…ë ¥ë°›ì•„ ê´€ë ¨ëœ ëª¨ë“  ì¶”ê°€ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜\n",
    "def extract_additional_features(pet_id, mode='train'):\n",
    "    \n",
    "    # Sentiment íŒŒì¼ ê²½ë¡œë¥¼ ìƒì„±\n",
    "    sentiment_filename = '../input/{}_sentiment/{}.json'.format(mode, pet_id)\n",
    "    try:\n",
    "        # íŒŒì¼ ì—´ê¸°\n",
    "        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n",
    "        # ë‚´ìš© íŒŒì‹±í•˜ì—¬ sentiment ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n",
    "        df_sentiment['PetID'] = pet_id\n",
    "    except FileNotFoundError: # íŒŒì¼ì„ ì°¾ì§€ ëª»í•˜ë©´\n",
    "        df_sentiment = [] # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬\n",
    "\n",
    "    # Metadata íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "    dfs_metadata = []\n",
    "    # í•´ë‹¹ PetIDë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  metadata íŒŒì¼ì„ ì°¾ìŒ (ì˜ˆ: a1b2c3-1.json, a1b2c3-2.json)\n",
    "    metadata_filenames = sorted(glob.glob('../input/{}_metadata/{}*.json'.format(mode, pet_id)))\n",
    "    \n",
    "    # ì°¾ì€ íŒŒì¼ì´ í•˜ë‚˜ ì´ìƒ ìˆë‹¤ë©´\n",
    "    if len(metadata_filenames) > 0:\n",
    "        # ê° íŒŒì¼ì„ ìˆœíšŒí•˜ë©°\n",
    "        for f in metadata_filenames:\n",
    "            # íŒŒì¼ ì—´ê¸°\n",
    "            metadata_file = pet_parser.open_metadata_file(f)\n",
    "            # ë‚´ìš© íŒŒì‹±í•˜ì—¬ metadata ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n",
    "            df_metadata['PetID'] = pet_id\n",
    "            # ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ dfs_metadata ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "            dfs_metadata.append(df_metadata)\n",
    "        # í•œ PetIDì— ì—¬ëŸ¬ metadata íŒŒì¼ì´ ìˆì„ ê²½ìš°, ì´ë“¤ì„ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í•©ì¹¨\n",
    "        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n",
    "    \n",
    "    # ìµœì¢…ì ìœ¼ë¡œ sentiment ê²°ê³¼ì™€ metadata ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ ë°˜í™˜\n",
    "    dfs = [df_sentiment, dfs_metadata]\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6172c1",
   "metadata": {},
   "source": [
    "2. extract_additional_features í•¨ìˆ˜: ì‹¤ì œ í–‰ë™ëŒ€ì¥\n",
    "ì´ í•¨ìˆ˜ëŠ” ìœ„ì—ì„œ ë§Œë“  PetFinderParserë¼ëŠ” ë„êµ¬ë¥¼ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "PetID í•˜ë‚˜ë¥¼ ê±´ë„¤ì£¼ë©´, í•´ë‹¹ IDì— ë§ëŠ” sentimentì™€ metadata íŒŒì¼ë“¤ì„ ì°¾ì•„ pet_parserì—ê²Œ ë„˜ê²¨ ì²˜ë¦¬ë¥¼ ë§¡ê¹ë‹ˆë‹¤.\n",
    "\n",
    "try...except êµ¬ë¬¸ì„ ì‚¬ìš©í•´ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°ì—ë„ ì˜¤ë¥˜ë¡œ ë©ˆì¶”ì§€ ì•Šê³  ìœ ì—°í•˜ê²Œ ëŒ€ì²˜í•˜ë©°, í•œ PetIDì— ì—¬ëŸ¬ ì´ë¯¸ì§€(ë©”íƒ€ë°ì´í„° íŒŒì¼)ê°€ ìˆëŠ” ê²½ìš°ë„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ í•¨ìˆ˜ê°€ ë°”ë¡œ Parallelì„ í†µí•´ ìˆ˜ì²œ ë²ˆì”© í˜¸ì¶œë  ì‹¤ì œ ì‘ì—… ë‹¨ìœ„ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95523c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ„ì—ì„œ ì„¤ê³„í•œ PetFinderParser í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤(ì‹¤ì²´)ë¥¼ ìƒì„±\n",
    "# ì´ì œ 'pet_parser'ë¼ëŠ” ë³€ìˆ˜ë¥¼ í†µí•´ í´ë˜ìŠ¤ ë‚´ì˜ ëª¨ë“  í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\n",
    "pet_parser = PetFinderParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8794c568",
   "metadata": {},
   "source": [
    "3. pet_parser = PetFinderParser(): ì„¤ê³„ë„ë¥¼ ì‹¤ì²´ë¡œ\n",
    "ë§ˆì§€ë§‰ ì¤„ì€ ìœ„ì—ì„œ ì •ì˜í•œ PetFinderParser ì„¤ê³„ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ pet_parserë¼ëŠ” ì‹¤ì œ \"ì¼ê¾¼\" ê°ì²´ë¥¼ ë§Œë“œëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ì œë¶€í„° pet_parser.open_metadata_file()ê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ í´ë˜ìŠ¤ ì•ˆì˜ í•¨ìˆ˜ë“¤ì„ í˜¸ì¶œí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. ì²˜ë¦¬í•  PetID ëª©ë¡ ì¤€ë¹„ ---\n",
    "# ë””ë²„ê¹… ëª¨ë“œ í”Œë˜ê·¸. Falseì¼ ê²½ìš° ì „ì²´ ë°ì´í„°ë¥¼, Trueì¼ ê²½ìš° ì¼ë¶€ ë°ì´í„°ë§Œ ì‚¬ìš©.\n",
    "debug = False\n",
    "# train, test ë°ì´í„°í”„ë ˆì„ì—ì„œ ê³ ìœ í•œ PetIDë§Œ ì¶”ì¶œí•˜ì—¬ ëª©ë¡ ìƒì„±\n",
    "train_pet_ids = train.PetID.unique()\n",
    "test_pet_ids = test.PetID.unique()\n",
    "\n",
    "# ë””ë²„ê·¸ ëª¨ë“œê°€ Trueì¼ ê²½ìš°, ì‹¤í–‰ ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ ì¼ë¶€ IDë§Œ ì„ íƒ\n",
    "if debug:\n",
    "    train_pet_ids = train_pet_ids[:1000] # train ID 1000ê°œ\n",
    "    test_pet_ids = test_pet_ids[:500]   # test ID 500ê°œ\n",
    "\n",
    "\n",
    "# --- 2. Train ì„¸íŠ¸ ì²˜ë¦¬ ---\n",
    "# Parallelì„ ì´ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘\n",
    "# n_jobs=6: 6ê°œì˜ CPU ì½”ì–´ë¥¼ ì‚¬ìš©\n",
    "# verbose=1: ì²˜ë¦¬ ì§„í–‰ ìƒí™©ì„ ê°„ë‹¨í•œ ë¡œê·¸ë¡œ í‘œì‹œ\n",
    "dfs_train = Parallel(n_jobs=6, verbose=1)(\n",
    "    # train_pet_idsì˜ ëª¨ë“  IDì— ëŒ€í•´ extract_additional_features í•¨ìˆ˜ë¥¼ ì‹¤í–‰\n",
    "    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids\n",
    ")\n",
    "\n",
    "# --- 3. Train ì„¸íŠ¸ ê²°ê³¼ ì •ë¦¬ ---\n",
    "# ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼(dfs_train)ì—ì„œ sentimentì™€ metadata ë°ì´í„°í”„ë ˆì„ì„ ê°ê° ë¶„ë¦¬\n",
    "# x[0]ì— ìˆëŠ” sentiment ê²°ê³¼ê°€ ë°ì´í„°í”„ë ˆì„ì¼ ê²½ìš°ì—ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "train_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\n",
    "# x[1]ì— ìˆëŠ” metadata ê²°ê³¼ê°€ ë°ì´í„°í”„ë ˆì„ì¼ ê²½ìš°ì—ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "train_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n",
    "\n",
    "# ë¶„ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ í° ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ê²°í•©(concat)\n",
    "train_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\n",
    "train_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n",
    "\n",
    "# ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ ë°ì´í„°í”„ë ˆì„ì˜ í˜•íƒœ(shape)ë¥¼ ì¶œë ¥ (í–‰ ìˆ˜, ì—´ ìˆ˜)\n",
    "print(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n",
    "\n",
    "\n",
    "# --- 4. Test ì„¸íŠ¸ ì²˜ë¦¬ ë° ì •ë¦¬ ---\n",
    "# (ìœ„ Train ì„¸íŠ¸ ì²˜ë¦¬ ê³¼ì •ê³¼ ì™„ì „íˆ ë™ì¼í•œ ë¡œì§ì„ Test ë°ì´í„°ì— ì ìš©)\n",
    "dfs_test = Parallel(n_jobs=6, verbose=1)(\n",
    "    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids\n",
    ")\n",
    "\n",
    "test_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\n",
    "test_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n",
    "\n",
    "test_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\n",
    "test_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n",
    "\n",
    "print(test_dfs_sentiment.shape, test_dfs_metadata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4506d95",
   "metadata": {},
   "source": [
    "Parallelì„ ì´ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ì²˜ë¦¬ ì„±ëŠ¥ í–¥ìƒ   \n",
    "\n",
    "4ê°œì˜ ë°ì´í„°ì…‹(train_sentiment_df, train_metadata_df, test_sentiment_df, test_metadata_df) ìƒì„±, ì´ 4ê°œì˜ dfëŠ” ì›ë³¸ train, test ë°ì´í„°ì— ìƒˆë¡œìš´ ì •ë³´ë¥¼ ë”í•´ì¤Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea2ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì•ìœ¼ë¡œ ì‚¬ìš©í•  ì§‘ê³„(aggregation) í•¨ìˆ˜ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì˜\n",
    "aggregates = ['mean', 'sum']\n",
    "\n",
    "\n",
    "# --- 1. Train ì„¸íŠ¸ - Metadata ì²˜ë¦¬ ---\n",
    "\n",
    "# -- 1a. í…ìŠ¤íŠ¸ ë°ì´í„°(annots_top_desc) ë¶„ë¦¬ ë° ì²˜ë¦¬ --\n",
    "# PetIDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ê³ , ê° PetIDì˜ ëª¨ë“  'metadata_annots_top_desc' í…ìŠ¤íŠ¸ë¥¼ ì¤‘ë³µ ì—†ì´ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ìŒ\n",
    "train_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\n",
    "train_metadata_desc = train_metadata_desc.reset_index() # ê·¸ë£¹í™”ëœ ì¸ë±ìŠ¤ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜\n",
    "# í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ë¡œ í•©ì¹¨\n",
    "train_metadata_desc[\n",
    "    'metadata_annots_top_desc'] = train_metadata_desc[\n",
    "    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# -- 1b. ìˆ˜ì¹˜ ë°ì´í„° ì²˜ë¦¬ --\n",
    "prefix = 'metadata' # ì»¬ëŸ¼ëª…ì— ì‚¬ìš©í•  ì ‘ë‘ì‚¬\n",
    "# ì›ë³¸ì—ì„œ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì œì™¸í•˜ì—¬ ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ë‚¨ê¹€\n",
    "train_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\n",
    "# PetIDë¥¼ ì œì™¸í•œ ëª¨ë“  ì»¬ëŸ¼ì„ float(ì‹¤ìˆ˜) íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (ì§‘ê³„ë¥¼ ìœ„í•´)\n",
    "for i in train_metadata_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\n",
    "# PetIDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ê³ , ê° ìˆ˜ì¹˜ ì»¬ëŸ¼ì— ëŒ€í•´ í‰ê· (mean)ê³¼ í•©ê³„(sum)ë¥¼ ê³„ì‚°\n",
    "train_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\n",
    "# ìƒì„±ëœ ë©€í‹°ì¸ë±ìŠ¤ ì»¬ëŸ¼ëª…ì„ 'prefix_ì›ë³¸ì»¬ëŸ¼ëª…_ì§‘ê³„í•¨ìˆ˜' í˜•ì‹ìœ¼ë¡œ ì¬ì •ì˜ (ì˜ˆ: metadata_annots_score_MEAN)\n",
    "train_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in train_metadata_gr.columns.tolist()])\n",
    "train_metadata_gr = train_metadata_gr.reset_index() # ê·¸ë£¹í™”ëœ ì¸ë±ìŠ¤ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜\n",
    "\n",
    "\n",
    "# --- 2. Train ì„¸íŠ¸ - Sentiment ì²˜ë¦¬ ---\n",
    "# (ìœ„ Metadata ì²˜ë¦¬ì™€ ì™„ì „íˆ ë™ì¼í•œ ê³¼ì •)\n",
    "\n",
    "train_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\n",
    "train_sentiment_desc = train_sentiment_desc.reset_index()\n",
    "train_sentiment_desc[\n",
    "    'sentiment_entities'] = train_sentiment_desc[\n",
    "    'sentiment_entities'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'sentiment'\n",
    "train_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\n",
    "for i in train_sentiment_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\n",
    "train_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(aggregates)\n",
    "train_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in train_sentiment_gr.columns.tolist()])\n",
    "train_sentiment_gr = train_sentiment_gr.reset_index()\n",
    "\n",
    "\n",
    "# --- 3. Test ì„¸íŠ¸ ì²˜ë¦¬ ---\n",
    "# (ìœ„ Train ì„¸íŠ¸ ì²˜ë¦¬ì™€ ì™„ì „íˆ ë™ì¼í•œ ê³¼ì •ì„ Test ë°ì´í„°ì— ì ìš©)\n",
    "\n",
    "# Metadata ì²˜ë¦¬\n",
    "test_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\n",
    "# ... (ì´í•˜ ë™ì¼)\n",
    "# ...\n",
    "\n",
    "# Sentiment ì²˜ë¦¬\n",
    "test_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\n",
    "# ... (ì´í•˜ ë™ì¼)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa6a50",
   "metadata": {},
   "source": [
    "- í•œ ë°˜ë ¤ë™ë¬¼ì´ ì—¬ëŸ¬ ê°œì˜ íŒŒì¼ì„ ê°€ì§ˆ ìˆ˜ ìˆìŒ -> í•˜ë‚˜ì˜ PetIDì— ì—¬ëŸ¬ í–‰ì˜ ë°ì´í„°ê°€ ìƒê¸°ê¸°ì—, ì›ë³¸ train ë°ì´í„°ì™€ í•©ì¹˜ê¸°ê°€ ê³¤ë€í•´ì§ˆ ìˆ˜ ìˆìŒ\n",
    "\n",
    "- í…ìŠ¤íŠ¸ì™€ ìˆ˜ì¹˜ ë°ì´í„°ê°€ ì„ì—¬ìˆê¸° ë•Œë¬¸ì— ê°ê° ë‹¤ë¥¸ ì „ëµ ì‚¬ìš©\n",
    "  1. í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ (_desc ì ‘ë¯¸ì‚¬)\n",
    "  ì—¬ëŸ¬ íŒŒì¼ì—ì„œ ë‚˜ì˜¨ í…ìŠ¤íŠ¸ë“¤ì„ .join() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í•˜ë‚˜ì˜ ê¸´ í…ìŠ¤íŠ¸ë¡œ ì´ì–´ ë¶™ì„\n",
    "  2. ìˆ˜ì¹˜ ë°ì´í„° ì²˜ë¦¬ (_gr ì ‘ë¯¸ì‚¬)\n",
    "  .agg(['mean', 'sum'])ì„ ì´ìš©í•´ ê° PetIDë³„ë¡œ ëª¨ë“  ìˆ˜ì¹˜ ì •ë³´ì˜ **í‰ê· (mean)ê³¼ í•©ê³„(sum)**ë¥¼ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Train ì„¸íŠ¸ ë°ì´í„° ë³‘í•©(Merge) ---\n",
    "\n",
    "# ì›ë³¸ train ë°ì´í„°í”„ë ˆì„ì˜ ë³µì‚¬ë³¸ì„ ë§Œë“¤ì–´ ì‘ì—… (ì›ë³¸ ë°ì´í„° ë³´ì¡´)\n",
    "train_proc = train.copy()\n",
    "# 'PetID'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, train_procì— ìˆ˜ì¹˜í˜• sentiment íŠ¹ì§•(_gr)ì„ 'left' ë°©ì‹ìœ¼ë¡œ ë³‘í•©\n",
    "train_proc = train_proc.merge(\n",
    "    train_sentiment_gr, how='left', on='PetID')\n",
    "# ì´ì–´ì„œ ìˆ˜ì¹˜í˜• metadata íŠ¹ì§•(_gr)ì„ ë³‘í•©\n",
    "train_proc = train_proc.merge(\n",
    "    train_metadata_gr, how='left', on='PetID')\n",
    "# ì´ì–´ì„œ í…ìŠ¤íŠ¸í˜• metadata íŠ¹ì§•(_desc)ì„ ë³‘í•©\n",
    "train_proc = train_proc.merge(\n",
    "    train_metadata_desc, how='left', on='PetID')\n",
    "# ë§ˆì§€ë§‰ìœ¼ë¡œ í…ìŠ¤íŠ¸í˜• sentiment íŠ¹ì§•(_desc)ì„ ë³‘í•©\n",
    "train_proc = train_proc.merge(\n",
    "    train_sentiment_desc, how='left', on='PetID')\n",
    "\n",
    "# --- 2. Test ì„¸íŠ¸ ë°ì´í„° ë³‘í•©(Merge) ---\n",
    "\n",
    "# ì›ë³¸ test ë°ì´í„°í”„ë ˆì„ì˜ ë³µì‚¬ë³¸ì„ ë§Œë“¤ì–´ ì‘ì—…\n",
    "test_proc = test.copy()\n",
    "# (ìœ„ Train ì„¸íŠ¸ ë³‘í•©ê³¼ ë™ì¼í•œ ê³¼ì •ì„ Test ë°ì´í„°ì— ì ìš©)\n",
    "test_proc = test_proc.merge(\n",
    "    test_sentiment_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_metadata_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_metadata_desc, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_sentiment_desc, how='left', on='PetID')\n",
    "\n",
    "\n",
    "# --- 3. ìµœì¢… ê²°ê³¼ í™•ì¸ ë° ê²€ì¦ ---\n",
    "\n",
    "# ëª¨ë“  íŠ¹ì§•ì´ ë³‘í•©ëœ ìµœì¢… ë°ì´í„°í”„ë ˆì„ì˜ í˜•íƒœ(shape)ë¥¼ ì¶œë ¥\n",
    "print(train_proc.shape, test_proc.shape)\n",
    "# ë³‘í•© í›„ train ë°ì´í„°ì˜ í–‰(row) ìˆ˜ê°€ ì›ë³¸ê³¼ ë™ì¼í•œì§€ í™•ì¸ (ë°ì´í„° ìœ ì‹¤/ì¤‘ë³µ ë°©ì§€)\n",
    "assert train_proc.shape[0] == train.shape[0]\n",
    "# ë³‘í•© í›„ test ë°ì´í„°ì˜ í–‰ ìˆ˜ê°€ ì›ë³¸ê³¼ ë™ì¼í•œì§€ í™•ì¸\n",
    "assert test_proc.shape[0] == test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041651fe",
   "metadata": {},
   "source": [
    "- Data Merging ë‹¨ê³„\n",
    "\n",
    "- **how='left'ì˜ ì¤‘ìš”ì„±**\n",
    "mergeë¥¼ ìˆ˜í–‰í•  ë•Œ how='left' ì˜µì…˜ì€ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤. SQLì˜ left joinê³¼ ì˜ë¯¸ëŠ” ê°™ë‹¤.\n",
    "\n",
    "- **assertë¥¼ ì´ìš©í•œ ê²€ì¦**\n",
    "assert êµ¬ë¬¸ì€ í”„ë¡œê·¸ë˜ë¨¸ì˜ \"ì•ˆì „ì¥ì¹˜\" ë˜ëŠ” \"ìë™ ì ê²€\" ê¸°ëŠ¥ì´ë‹¤. assert ë’¤ì˜ ì¡°ê±´ì´ Trueê°€ ì•„ë‹ˆë©´, AssertionErrorê°€ ë°œìƒ\n",
    "\n",
    "ì—¬ê¸°ì„œ assert train_proc.shape[0] == train.shape[0]ëŠ” \"ë³‘í•© í›„ ë°ì´í„°ì˜ í–‰ ìˆ˜ê°€ ì›ë³¸ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ë§Œ í•œë‹¤\"ëŠ” ê²ƒì„ ê°•ë ¥í•˜ê²Œ í™•ì¸í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ ë§Œì•½ ë³‘í•© ê³¼ì •ì—ì„œ ë¬´ì–¸ê°€ ì˜ëª»ë˜ì–´ ë°ì´í„° í–‰ì´ ì¤‘ë³µë˜ê±°ë‚˜ ì‚¬ë¼ì¡Œë‹¤ë©´, ì´ assert êµ¬ë¬¸ì´ ì¦‰ì‹œ ë¬¸ì œë¥¼ ì•Œë ¤ì£¼ì–´ ë²„ê·¸ë¥¼ ì¡°ê¸°ì— ë°œê²¬í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ\n",
    "\n",
    "ì´ ë‹¨ê³„ë¥¼ ê±°ì³ ìƒì„±ëœ train_procì™€ test_procëŠ” ëª¨ë“  íŠ¹ì§•ì´ ê²°í•©ëœ, ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ìµœì¢… ì¤€ë¹„ë¥¼ ë§ˆì¹œ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03118bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Train ì„¸íŠ¸ì˜ í’ˆì¢…(Breed) ì •ë³´ ë³€í™˜ ---\n",
    "\n",
    "# -- 1a. ì²« ë²ˆì§¸ í’ˆì¢…(Breed1) ì²˜ë¦¬ --\n",
    "# 'Breed1' ì»¬ëŸ¼ê³¼ í’ˆì¢… ì´ë¦„ì´ ë‹´ê¸´ 'labels_breed'ë¥¼ ë³‘í•©\n",
    "train_breed_main = train_proc[['Breed1']].merge(\n",
    "    labels_breed, how='left',         # 'Breed1'ì˜ ëª¨ë“  ê°’ì„ ìœ ì§€\n",
    "    left_on='Breed1', right_on='BreedID', # 'Breed1'ê³¼ 'BreedID'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©\n",
    "    suffixes=('', '_main_breed'))     # ì¤‘ë³µ ì»¬ëŸ¼ëª…ì— ì ‘ë¯¸ì‚¬ ì¶”ê°€\n",
    "\n",
    "# ë³‘í•© ê²°ê³¼ì—ì„œ ë¶ˆí•„ìš”í•œ ID ì»¬ëŸ¼ë“¤(Breed1, BreedID)ì„ ì œì™¸í•˜ê³ , ì‹¤ì œ ì •ë³´ê°€ ë‹´ê¸´ ì»¬ëŸ¼ë§Œ ë‚¨ê¹€\n",
    "train_breed_main = train_breed_main.iloc[:, 2:]\n",
    "# ë‚¨ì€ ì»¬ëŸ¼ë“¤ì˜ ì´ë¦„ ì•ì— 'main_breed_' ì ‘ë‘ì‚¬ë¥¼ ë¶™ì—¬ ì²« ë²ˆì§¸ í’ˆì¢… ì •ë³´ì„ì„ ëª…ì‹œ\n",
    "train_breed_main = train_breed_main.add_prefix('main_breed_')\n",
    "\n",
    "# -- 1b. ë‘ ë²ˆì§¸ í’ˆì¢…(Breed2) ì²˜ë¦¬ (ìœ„ì™€ ë™ì¼í•œ ê³¼ì • ë°˜ë³µ) --\n",
    "train_breed_second = train_proc[['Breed2']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed2', right_on='BreedID',\n",
    "    suffixes=('', '_second_breed'))\n",
    "\n",
    "train_breed_second = train_breed_second.iloc[:, 2:]\n",
    "train_breed_second = train_breed_second.add_prefix('second_breed_')\n",
    "\n",
    "\n",
    "# -- 1c. ë³€í™˜ëœ í’ˆì¢… ì •ë³´ë¥¼ ì›ë³¸ ë°ì´í„°ì— ìµœì¢… ê²°í•© --\n",
    "# ê¸°ì¡´ train_proc ë°ì´í„°í”„ë ˆì„ ì˜†ì—(axis=1) main_breedì™€ second_breed ì •ë³´ë¥¼ ë¶™ì„\n",
    "train_proc = pd.concat(\n",
    "    [train_proc, train_breed_main, train_breed_second], axis=1)\n",
    "\n",
    "\n",
    "# --- 2. Test ì„¸íŠ¸ì˜ í’ˆì¢…(Breed) ì •ë³´ ë³€í™˜ ---\n",
    "# (ìœ„ Train ì„¸íŠ¸ ì²˜ë¦¬ì™€ ì™„ì „íˆ ë™ì¼í•œ ê³¼ì •ì„ Test ë°ì´í„°ì— ì ìš©)\n",
    "\n",
    "test_breed_main = test_proc[['Breed1']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed1', right_on='BreedID',\n",
    "    suffixes=('', '_main_breed'))\n",
    "# ... (ì´í•˜ ë™ì¼)\n",
    "\n",
    "\n",
    "# --- 3. ìµœì¢… ê²°ê³¼ í™•ì¸ ---\n",
    "# í’ˆì¢… ì •ë³´ê°€ ì¶”ê°€ëœ ìµœì¢… ë°ì´í„°í”„ë ˆì„ì˜ í˜•íƒœ(shape)ë¥¼ ì¶œë ¥\n",
    "print(train_proc.shape, test_proc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f0a46",
   "metadata": {},
   "source": [
    "- ë¶€ê°€ ì„¤ëª…\n",
    "ì´ ì½”ë“œ ë¸”ë¡ì˜ í•µì‹¬ ëª©í‘œëŠ” ì˜ë¯¸ ì—†ëŠ” ìˆ«ì IDë¥¼ í•´ì„ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì •ë³´ë¡œ ë³€í™˜í•˜ëŠ” íŠ¹ì§• ìƒì„±(Feature Creation) ê³¼ì •ì…ë‹ˆë‹¤.\n",
    "\n",
    "train ë°ì´í„°ì— ìˆëŠ” Breed1, Breed2 ì»¬ëŸ¼ì€ '307'ì´ë‚˜ '266' ê°™ì€ ìˆ«ìë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. ì´ ìˆ«ì ìì²´ëŠ” ëª¨ë¸ì—ê²Œ ì•„ë¬´ëŸ° ì˜ë¯¸ë¥¼ ì£¼ì§€ ëª»í•©ë‹ˆë‹¤. ì´ ì½”ë“œëŠ” labels_breed.csvë¼ëŠ” **ì°¸ì¡°í‘œ(Lookup Table)**ë¥¼ ì´ìš©í•´ ì´ ìˆ«ì IDë“¤ì„ 'Mixed Breed', 'Domestic Short Hair' ê°™ì€ ì‹¤ì œ í’ˆì¢… ì´ë¦„ìœ¼ë¡œ ë°”ê¾¸ì–´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ì²˜ë¦¬ ê³¼ì • ìš”ì•½\n",
    "ì •ë³´ ë§¤ì¹­ (merge): train_procì˜ Breed1 IDë¥¼ labels_breedì˜ BreedIDì™€ ë¹„êµí•˜ì—¬ ì¼ì¹˜í•˜ëŠ” í’ˆì¢… ì´ë¦„(BreedName)ê³¼ íƒ€ì…(Type) ì •ë³´ë¥¼ ì°¾ì•„ ì˜†ì— ë¶™ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì •ë¦¬ ë° ì´ë¦„ ë¶€ì—¬ (iloc, add_prefix):\n",
    "\n",
    "ë³‘í•©ì— ì‚¬ìš©ëœ ë¶ˆí•„ìš”í•œ ID ì»¬ëŸ¼ë“¤ì„ ilocìœ¼ë¡œ ì œê±°í•˜ì—¬ ìˆœìˆ˜í•œ í’ˆì¢… ì •ë³´ë§Œ ë‚¨ê¹ë‹ˆë‹¤.\n",
    "\n",
    "ì²« ë²ˆì§¸ í’ˆì¢…(Breed1)ì—ì„œ ì˜¨ ì •ë³´ì¸ì§€, ë‘ ë²ˆì§¸ í’ˆì¢…(Breed2)ì—ì„œ ì˜¨ ì •ë³´ì¸ì§€ ëª…í™•íˆ êµ¬ë¶„í•˜ê¸° ìœ„í•´, ê° ì»¬ëŸ¼ëª… ì•ì— main_breed_, second_breed_ ì™€ ê°™ì€ ì ‘ë‘ì‚¬ë¥¼ ë¶™ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ë‚˜ì¤‘ì— ëª¨ë¸ ë¶„ì„ ì‹œ í˜¼ë™ì„ ë§‰ì•„ì¤ë‹ˆë‹¤.\n",
    "\n",
    "ìµœì¢… ê²°í•© (concat):\n",
    "\n",
    "ì´ë ‡ê²Œ ê¹”ë”í•˜ê²Œ ì •ë¦¬ëœ í’ˆì¢… ì´ë¦„ ì •ë³´(train_breed_main, train_breed_second)ë¥¼ ì›ë˜ì˜ train_proc ë°ì´í„°í”„ë ˆì„ì˜ ì˜¤ë¥¸ìª½ì— ì—´(column) ë°©í–¥ìœ¼ë¡œ ì´ì–´ ë¶™ì…ë‹ˆë‹¤ (axis=1).\n",
    "\n",
    "ì´ ê³¼ì •ì„ í†µí•´ ê¸°ì¡´ì˜ ìˆ«ì ID ì»¬ëŸ¼ ì™¸ì—, ëª¨ë¸ì´ ë” ì˜ ì´í•´í•˜ê³  í™œìš©í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ íŠ¹ì§•(feature) ì»¬ëŸ¼ë“¤ì´ ë°ì´í„°ì…‹ì— ì¶”ê°€ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)\n",
    "print('NaN structure:\\n{}'.format(np.sum(pd.isnull(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba653029",
   "metadata": {},
   "source": [
    "- ë³¸ê²©ì ì¸ ëª¨ë¸ë§ì— ì•ì„œ ë°ì´í„°ë¥¼ ìµœì¢…ì ìœ¼ë¡œ í†µí•©í•˜ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ ì ê²€\n",
    "\n",
    "- ì™œ í•©ì³¤ëŠ”ê°€?   \n",
    "\n",
    "  - ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ê¸° ì „, ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬(ì˜ˆ: í…ìŠ¤íŠ¸ ì¸ì½”ë”©, ìˆ«ì ìŠ¤ì¼€ì¼ë§ ë“±)í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë•Œ train ë°ì´í„°ì™€ test ë°ì´í„°ëŠ” ë°˜ë“œì‹œ ë™ì¼í•œ ê¸°ì¤€ê³¼ ë°©ë²•ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "  - ë§Œì•½ ë‘ ë°ì´í„°ë¥¼ ë”°ë¡œë”°ë¡œ ì „ì²˜ë¦¬í•˜ë©´, ê¸°ì¤€ì´ ë¯¸ì„¸í•˜ê²Œ ë‹¬ë¼ì ¸ ëª¨ë¸ ì„±ëŠ¥ì— ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "  - ê·¸ë˜ì„œ ê°€ì¥ íš¨ìœ¨ì ì´ê³  ì•ˆì „í•œ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "    1. ì¼ë‹¨ í•©ì¹œë‹¤ (pd.concat): trainê³¼ testë¥¼ í•˜ë‚˜ì˜ í° ë°ì´í„°(X)ë¡œ í•©ì¹©ë‹ˆë‹¤.\n",
    "    2. í•œ ë²ˆì— ì²˜ë¦¬í•œë‹¤: í•©ì³ì§„ ë°ì´í„° Xì— ëª¨ë“  ì „ì²˜ë¦¬ ê³¼ì •ì„ í•œêº¼ë²ˆì— ì ìš©í•©ë‹ˆë‹¤.\n",
    "    3. ë‹¤ì‹œ ë‚˜ëˆˆë‹¤: ì „ì²˜ë¦¬ê°€ ëë‚˜ë©´, AdoptionSpeed ì—´ì— ê°’ì´ ìˆëŠ”ì§€ ì—†ëŠ”ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ X_trainê³¼ X_testë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51823df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. ë°ì´í„° íƒ€ì…ë³„ë¡œ ì»¬ëŸ¼ ë¶„ë¥˜ ---\n",
    "# í†µí•© ë°ì´í„°í”„ë ˆì„ Xì˜ ê° ì»¬ëŸ¼ë³„ ë°ì´í„° íƒ€ì…(dtype)ì„ í™•ì¸\n",
    "column_types = X.dtypes\n",
    "\n",
    "# ë°ì´í„° íƒ€ì…ì´ ì •ìˆ˜(int)ì¸ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒ\n",
    "int_cols = column_types[column_types == 'int']\n",
    "# ë°ì´í„° íƒ€ì…ì´ ì‹¤ìˆ˜(float)ì¸ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒ\n",
    "float_cols = column_types[column_types == 'float']\n",
    "# ë°ì´í„° íƒ€ì…ì´ ê°ì²´(object)ì¸ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒ (ì£¼ë¡œ ë¬¸ìì—´ ë°ì´í„°)\n",
    "cat_cols = column_types[column_types == 'object']\n",
    "\n",
    "# --- 2. ë¶„ë¥˜ ê²°ê³¼ ì¶œë ¥ ---\n",
    "# ë¶„ë¥˜ëœ ì •ìˆ˜í˜• ì»¬ëŸ¼ë“¤ì˜ ëª©ë¡ì„ ì¶œë ¥\n",
    "print('\\tinteger columns:\\n{}'.format(int_cols))\n",
    "# ë¶„ë¥˜ëœ ì‹¤ìˆ˜í˜• ì»¬ëŸ¼ë“¤ì˜ ëª©ë¡ì„ ì¶œë ¥\n",
    "print('\\n\\tfloat columns:\\n{}'.format(float_cols))\n",
    "# ë¶„ë¥˜ëœ ë²”ì£¼í˜•(object) ì»¬ëŸ¼ë“¤ì˜ ëª©ë¡ì„ ì¶œë ¥. ì´ ì»¬ëŸ¼ë“¤ì€ ì¸ì½”ë”©ì´ í•„ìš”í•¨.\n",
    "print('\\n\\tto encode categorical columns:\\n{}'.format(cat_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411fc38",
   "metadata": {},
   "source": [
    "- ê° ì»¬ëŸ¼ë“¤ì˜ ë°ì´í„° íƒ€ì…ì„ êµ¬ë¶„í•´ë‘ë©´ í›„ì²˜ë¦¬ê°€ ì‰¬ì›Œì§„ë‹µë‹ˆë‹¤~ (ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ì—ëŠ” ìŠ¤ì¼€ì¼ë§ì„, ë²”ì£¼í˜• ì»¬ëŸ¼ë“¤ì—ëŠ” ì¸ì½”ë”©ì„ ì ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original X DF for easier experimentation,\n",
    "# all feature engineering will be performed on this one:\n",
    "X_temp = X.copy()\n",
    "\n",
    "\n",
    "# Select subsets of columns:\n",
    "text_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\n",
    "categorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n",
    "\n",
    "# Names are all unique, so they can be dropped by default\n",
    "# Same goes for PetID, it shouldn't be used as a feature\n",
    "to_drop_columns = ['PetID', 'Name', 'RescuerID']\n",
    "# RescuerID will also be dropped, as a feature based on this column will be extracted independently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ae482",
   "metadata": {},
   "source": [
    "- text_columns: ìì—°ì–´ ì²˜ë¦¬(NLP) ê¸°ìˆ (ì˜ˆ: TF-IDF)ì„ ì ìš©í•´ì•¼ í•˜ëŠ” ì»¬ëŸ¼ë“¤ì…ë‹ˆë‹¤.\n",
    "\n",
    "- categorical_columns: í’ˆì¢… ì´ë¦„ì²˜ëŸ¼ ì •í•´ì§„ ëª‡ ê°œì˜ ì¹´í…Œê³ ë¦¬ë¡œ ì´ë£¨ì–´ì§„ ì»¬ëŸ¼ë“¤ì…ë‹ˆë‹¤. ì´ë“¤ì€ ì£¼ë¡œ ì›-í•« ì¸ì½”ë”©(One-Hot Encoding)ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.\n",
    "\n",
    "- to_drop_columns: ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì— ë„ì›€ì´ ë˜ì§€ ì•Šê±°ë‚˜, ì˜¤íˆë ¤ ë°©í•´ê°€ ë  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ë“¤ì„ ë¯¸ë¦¬ ì§€ì •í•©ë‹ˆë‹¤. PetIDë‚˜ Nameì²˜ëŸ¼ ëª¨ë“  í–‰ë§ˆë‹¤ ê°’ì´ ë‹¤ë¥¸ ê³ ìœ  ì‹ë³„ìëŠ” ëª¨ë¸ì´ ì•”ê¸°í•˜ê²Œ ë§Œë“¤ì–´ ê³¼ì í•©(overfitting)ì„ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count RescuerID occurrences:\n",
    "rescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\n",
    "rescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n",
    "\n",
    "# Merge as another feature onto main DF:\n",
    "X_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a89be1",
   "metadata": {},
   "source": [
    "- RescuerID_COUNT ë¼ëŠ” íŒŒìƒ ë³€ìˆ˜ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize categorical columns:\n",
    "for i in categorical_columns:\n",
    "    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51fcb25",
   "metadata": {},
   "source": [
    "- ë¼ë²¨ì¸ì½”ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cff653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset text features:\n",
    "X_text = X_temp[text_columns]\n",
    "\n",
    "for i in X_text.columns:\n",
    "    X_text.loc[:, i] = X_text.loc[:, i].fillna('<MISSING>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf91604",
   "metadata": {},
   "source": [
    "ì´ ì½”ë“œ ë¸”ë¡ì€ **ìì—°ì–´ ì²˜ë¦¬(NLP)**ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë”°ë¡œ ì¤€ë¹„í•˜ê³ , ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ê²°ì¸¡ì¹˜ë¥¼ ê´€ë¦¬í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.\n",
    "\n",
    "í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬: ì „ì²´ ë°ì´í„°(X_temp)ì—ì„œ í…ìŠ¤íŠ¸ ë¶„ì„ì´ í•„ìš”í•œ ì»¬ëŸ¼ë“¤ë§Œ ê³¨ë¼ X_textë¼ëŠ” ë³„ë„ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì€ íš¨ìœ¨ì ì¸ ì‘ì—… ë°©ì‹ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ìˆ«ìë‚˜ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ë°ì´í„°ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šê³  í…ìŠ¤íŠ¸ ê´€ë ¨ ì „ì²˜ë¦¬(ì˜ˆ: TF-IDF)ë¥¼ ê¹”ë”í•˜ê²Œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê²°ì¸¡ì¹˜ ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±: í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬(ì˜ˆ: TfidfVectorizer)ëŠ” NaNê³¼ ê°™ì€ ë¹„ì–´ìˆëŠ” ê°’ì„ ë§Œë‚˜ë©´ ì˜¤ë¥˜ë¥¼ ì¼ìœ¼í‚µë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë“  ê°’ì„ ë¬¸ìì—´ í˜•íƒœë¡œ ë§Œë“¤ì–´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. .fillna('<MISSING>')ì€ ì´ëŸ¬í•œ NaN ê°’ë“¤ì„ '<MISSING>'ì´ë¼ëŠ” ì¼ê´€ëœ ë¬¸ìì—´ë¡œ ëŒ€ì²´í•˜ì—¬ í›„ì† ì‘ì—…ì´ ì›í™œí•˜ê²Œ ì§„í–‰ë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë˜í•œ, ë‹¨ìˆœíˆ ë¹ˆì¹¸('')ìœ¼ë¡œ ì±„ìš°ëŠ” ëŒ€ì‹  '<MISSING>'ì´ë¼ëŠ” ì˜ë¯¸ ìˆëŠ” ë¬¸ìì—´ë¡œ ì±„ìš°ë©´, \"ì„¤ëª…ì´ ì—†ëŠ” ê²½ìš°\" ìì²´ê°€ í•˜ë‚˜ì˜ ì¤‘ìš”í•œ íŠ¹ì§•(feature)ìœ¼ë¡œ ì‘ìš©í•˜ì—¬ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14056b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF # ì°¨ì› ì¶•ì†Œ ê¸°ë²•\n",
    "\n",
    "# --- 2. íŒŒë¼ë¯¸í„° ë° ë³€ìˆ˜ ì´ˆê¸°í™” ---\n",
    "n_components = 5      # ê° í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì—ì„œ ì¶”ì¶œí•  ì£¼ìš” í† í”½(topic) ë˜ëŠ” íŠ¹ì§•(feature)ì˜ ê°œìˆ˜\n",
    "text_features = []    # ìƒì„±ëœ í…ìŠ¤íŠ¸ íŠ¹ì§•ë“¤ì„ ì„ì‹œë¡œ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# --- 3. ê° í…ìŠ¤íŠ¸ ì»¬ëŸ¼ë³„ íŠ¹ì§• ìƒì„± ---\n",
    "# ì´ì „ì— ì¤€ë¹„í•œ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ë“¤('Description' ë“±)ì— ëŒ€í•´ ë°˜ë³µ\n",
    "for i in X_text.columns:\n",
    "    \n",
    "    # -- 3a. ì•Œê³ ë¦¬ì¦˜ ì´ˆê¸°í™” --\n",
    "    print('generating features from: {}'.format(i))\n",
    "    # Truncated SVD ëª¨ë¸ ì´ˆê¸°í™” (n_componentsê°œì˜ íŠ¹ì§• ì¶”ì¶œ)\n",
    "    svd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n",
    "    # NMF ëª¨ë¸ ì´ˆê¸°í™” (n_componentsê°œì˜ íŠ¹ì§• ì¶”ì¶œ)\n",
    "    nmf_ = NMF(n_components=n_components, random_state=1337)\n",
    "    \n",
    "    # -- 3b. TF-IDF ë²¡í„°í™” --\n",
    "    # TfidfVectorizerë¥¼ ì´ìš©í•´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜ ë²¡í„°(TF-IDF í–‰ë ¬)ë¡œ ë³€í™˜\n",
    "    tfidf_col = TfidfVectorizer().fit_transform(X_text.loc[:, i].values)\n",
    "    \n",
    "    # -- 3c. SVD ì ìš© ë° ë°ì´í„°í”„ë ˆì„ ë³€í™˜ --\n",
    "    # TF-IDF í–‰ë ¬ì— SVDë¥¼ ì ìš©í•˜ì—¬ n_componentsê°œì˜ ì£¼ìš” íŠ¹ì§•ìœ¼ë¡œ ì••ì¶•\n",
    "    svd_col = svd_.fit_transform(tfidf_col)\n",
    "    svd_col = pd.DataFrame(svd_col) # ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "    svd_col = svd_col.add_prefix('SVD_{}_'.format(i)) # ì»¬ëŸ¼ëª…ì— ì ‘ë‘ì‚¬ë¥¼ ë¶™ì—¬ êµ¬ë¶„\n",
    "    \n",
    "    # -- 3d. NMF ì ìš© ë° ë°ì´í„°í”„ë ˆì„ ë³€í™˜ --\n",
    "    # TF-IDF í–‰ë ¬ì— NMFë¥¼ ì ìš©í•˜ì—¬ n_componentsê°œì˜ ì£¼ìš” íŠ¹ì§•ìœ¼ë¡œ ì••ì¶•\n",
    "    nmf_col = nmf_.fit_transform(tfidf_col)\n",
    "    nmf_col = pd.DataFrame(nmf_col) # ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i)) # ì»¬ëŸ¼ëª…ì— ì ‘ë‘ì‚¬ë¥¼ ë¶™ì—¬ êµ¬ë¶„\n",
    "    \n",
    "    # -- 3e. ê²°ê³¼ ì €ì¥ --\n",
    "    # ìƒì„±ëœ SVDì™€ NMF íŠ¹ì§•ì„ text_features ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    text_features.append(svd_col)\n",
    "    text_features.append(nmf_col)\n",
    "\n",
    "    \n",
    "# --- 4. ëª¨ë“  í…ìŠ¤íŠ¸ íŠ¹ì§• ê²°í•© ---\n",
    "# text_features ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ëœ ëª¨ë“  ë°ì´í„°í”„ë ˆì„ ì¡°ê°ë“¤ì„ ì˜†ìœ¼ë¡œ ì´ì–´ ë¶™ì„ (axis=1)\n",
    "text_features = pd.concat(text_features, axis=1)\n",
    "\n",
    "# --- 5. ìµœì¢… ë°ì´í„°í”„ë ˆì„ì— ë³‘í•© ë° ì›ë³¸ í…ìŠ¤íŠ¸ ì œê±° ---\n",
    "# ìƒì„±ëœ í…ìŠ¤íŠ¸ íŠ¹ì§•ë“¤ì„ ë©”ì¸ ë°ì´í„°í”„ë ˆì„(X_temp)ì— ê²°í•©\n",
    "X_temp = pd.concat([X_temp, text_features], axis=1)\n",
    "\n",
    "# ì›ë³¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ë“¤ì€ ì´ì œ ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ì œê±°\n",
    "for i in X_text.columns:\n",
    "    X_temp = X_temp.drop(i, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f327c274",
   "metadata": {},
   "source": [
    "ì´ ì½”ë“œ ë¸”ë¡ì€ **ìì—°ì–´ ì²˜ë¦¬(NLP)**ì˜ í•µì‹¬ì ì¸ ë¶€ë¶„ìœ¼ë¡œ, ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ì—†ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ë¯¸ ìˆëŠ” ìˆ«ìí˜• íŠ¹ì§•(feature)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì€ í¬ê²Œ ë‘ ë‹¨ê³„ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.\n",
    "\n",
    "1. í…ìŠ¤íŠ¸ì˜ ìˆ˜ì¹˜í™”: TfidfVectorizer\n",
    "ì»´í“¨í„°ëŠ” 'ê³ ì–‘ì´'ë¼ëŠ” ë‹¨ì–´ ìì²´ë¥¼ ì´í•´í•˜ì§€ ëª»í•©ë‹ˆë‹¤. **TF-IDF(Term Frequency-Inverse Document Frequency)**ëŠ” ê° í…ìŠ¤íŠ¸(ë¬¸ì„œ)ì—ì„œ ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ì—¬, í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê±°ëŒ€í•œ ìˆ«ì í–‰ë ¬ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "TF (Term Frequency): í•œ ë¬¸ì„œ ì•ˆì—ì„œ íŠ¹ì • ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ìì£¼ ë“±ì¥í•˜ëŠ”ì§€.\n",
    "\n",
    "IDF (Inverse Document Frequency): íŠ¹ì • ë‹¨ì–´ê°€ ì „ì²´ ë¬¸ì„œë“¤ ì¤‘ì—ì„œ ì–¼ë§ˆë‚˜ í¬ê·€í•˜ê²Œ ë“±ì¥í•˜ëŠ”ì§€. (ëª¨ë“  ë¬¸ì„œì— ë‹¤ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ëŠ” ì¤‘ìš”ë„ê°€ ë‚®ìŒ)\n",
    "\n",
    "ì´ ê³¼ì •ì„ ê±°ì¹˜ë©´ ê° í…ìŠ¤íŠ¸ëŠ” ìˆ˜ë§Œ ê°œì˜ ë‹¨ì–´(ì»¬ëŸ¼)ë¡œ ì´ë£¨ì–´ì§„ ë§¤ìš° í° ìˆ«ì ë²¡í„°ê°€ ë©ë‹ˆë‹¤.\n",
    "\n",
    "2. ì •ë³´ ì••ì¶• ë° í† í”½ ì¶”ì¶œ: TruncatedSVD & NMF\n",
    "TF-IDFë¡œ ë³€í™˜ëœ ë°ì´í„°ëŠ” ë„ˆë¬´ í¬ê³ (ìˆ˜ë§Œ ê°œì˜ ì»¬ëŸ¼) ì •ë³´ê°€ í©ì–´ì ¸ ìˆì–´ ëª¨ë¸ì´ í•™ìŠµí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì°¨ì› ì¶•ì†Œ(Dimensionality Reduction) ê¸°ë²•ì€ ì´ ë°©ëŒ€í•œ ì •ë³´ë¥¼ ëª‡ ê°œì˜ í•µì‹¬ì ì¸ 'í† í”½(Topic)' ë˜ëŠ” **'ì ì¬ ì˜ë¯¸(Latent Semantic)'**ë¡œ ì••ì¶•í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "TruncatedSVD (íŠ¹ì´ê°’ ë¶„í•´): í…ìŠ¤íŠ¸ì— ìˆ¨ì–´ìˆëŠ” ì£¼ìš” íŒ¨í„´ê³¼ ì˜ë¯¸ë¥¼ ì°¾ì•„ë‚´ì–´, n_componentsê°œ(ì—¬ê¸°ì„œëŠ” 5ê°œ)ì˜ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§•ìœ¼ë¡œ ì •ë³´ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.\n",
    "\n",
    "NMF (ìŒìˆ˜ ë¯¸í¬í•¨ í–‰ë ¬ ë¶„í•´): SVDì™€ ìœ ì‚¬í•˜ê²Œ í† í”½ì„ ì¶”ì¶œí•˜ì§€ë§Œ, ëª¨ë“  ê°’ì´ ì–‘ìˆ˜ì—¬ì•¼ í•œë‹¤ëŠ” ì œì•½ ì¡°ê±´ì´ ìˆì–´ ì¢€ ë” í•´ì„ì´ ì‰¬ìš´ ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê²°ê³¼ì ìœ¼ë¡œ, í•˜ë‚˜ì˜ ê¸´ ë°˜ë ¤ë™ë¬¼ ì„¤ëª…ë¬¸ì€ \"í™œë°œí•¨ì— ëŒ€í•œ ì ìˆ˜\", \"ì¹œê·¼í•¨ì— ëŒ€í•œ ì ìˆ˜\" ë“±ê³¼ ê°™ì´ 5ê°œì˜ ì£¼ìš” í† í”½ ì ìˆ˜ë¡œ ì••ì¶•ë©ë‹ˆë‹¤. ì´ ì½”ë“œì—ì„œëŠ” SVDì™€ NMF ë‘ ê°€ì§€ ê¸°ë²•ì„ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì ì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê³¼ì •ì„ ê±°ì³ ì›ë³¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ë“¤ì€ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” 10ê°œ(= 5ê°œ(SVD) + 5ê°œ(NMF))ì˜ ìƒˆë¡œìš´ ìˆ«ìí˜• íŠ¹ì§• ì»¬ëŸ¼ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.\n",
    "\n",
    "- \"ì˜ë¯¸ë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” ê¸´ í…ìŠ¤íŠ¸\" â†’ TF-IDF â†’ \"ìˆ˜ë§Œ ê°œì˜ ë‹¨ì–´ ì ìˆ˜ ë²¡í„°\" â†’ SVD/NMF â†’ \"ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” 5ê°œì˜ í•µì‹¬ í† í”½ ì ìˆ˜\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns:\n",
    "X_temp = X_temp.drop(to_drop_columns, axis=1)\n",
    "\n",
    "# Check final df shape:\n",
    "print('X shape: {}'.format(X_temp.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bbf5b4",
   "metadata": {},
   "source": [
    "- ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e92560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. ë°ì´í„°ë¥¼ ë‹¤ì‹œ Trainê³¼ Testë¡œ ë¶„ë¦¬ ---\n",
    "\n",
    "# 'AdoptionSpeed' ì»¬ëŸ¼ì— ìœ í•œí•œ ê°’(ìˆ«ì)ì´ ìˆëŠ” í–‰ë“¤ì„ X_trainìœ¼ë¡œ ì„ íƒ (ì›ë³¸ train ë°ì´í„°)\n",
    "X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\n",
    "# 'AdoptionSpeed' ì»¬ëŸ¼ì— ìœ í•œí•œ ê°’ì´ ì—†ëŠ” í–‰(NaN)ë“¤ì„ X_testë¡œ ì„ íƒ (ì›ë³¸ test ë°ì´í„°)\n",
    "X_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n",
    "\n",
    "# --- 2. Test ì„¸íŠ¸ì—ì„œ ì •ë‹µ ì»¬ëŸ¼ ì œê±° ---\n",
    "\n",
    "# X_test ë°ì´í„°í”„ë ˆì„ì—ëŠ” ì •ë‹µì´ ì—†ì–´ì•¼ í•˜ë¯€ë¡œ, 'AdoptionSpeed' ì»¬ëŸ¼ì„ ì œê±°\n",
    "X_test = X_test.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "\n",
    "# --- 3. ë¶„ë¦¬ ê²°ê³¼ í™•ì¸ ë° ê²€ì¦ ---\n",
    "\n",
    "# ìµœì¢…ì ìœ¼ë¡œ ë¶„ë¦¬ëœ X_trainê³¼ X_testì˜ í˜•íƒœ(shape)ë¥¼ ì¶œë ¥\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('X_test shape: {}'.format(X_test.shape))\n",
    "\n",
    "# ë¶„ë¦¬ëœ X_trainì˜ í–‰ ìˆ˜ê°€ ì›ë³¸ trainì˜ í–‰ ìˆ˜ì™€ ë™ì¼í•œì§€ í™•ì¸\n",
    "assert X_train.shape[0] == train.shape[0]\n",
    "# ë¶„ë¦¬ëœ X_testì˜ í–‰ ìˆ˜ê°€ ì›ë³¸ testì˜ í–‰ ìˆ˜ì™€ ë™ì¼í•œì§€ í™•ì¸\n",
    "assert X_test.shape[0] == test.shape[0]\n",
    "\n",
    "\n",
    "# --- 4. ë‘ ë°ì´í„°í”„ë ˆì„ì˜ ì»¬ëŸ¼ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸ ---\n",
    "\n",
    "# X_trainì˜ ì»¬ëŸ¼ ëª©ë¡ì—ì„œ ì •ë‹µ('AdoptionSpeed') ì»¬ëŸ¼ì„ ì œì™¸\n",
    "train_cols = X_train.columns.tolist()\n",
    "train_cols.remove('AdoptionSpeed')\n",
    "\n",
    "# X_testì˜ ì»¬ëŸ¼ ëª©ë¡ì„ ê°€ì ¸ì˜´\n",
    "test_cols = X_test.columns.tolist()\n",
    "\n",
    "# ë‘ ì»¬ëŸ¼ ëª©ë¡ì´ ìˆœì„œê¹Œì§€ ì™„ë²½í•˜ê²Œ ë™ì¼í•œì§€ í™•ì¸\n",
    "assert np.all(train_cols == test_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341bfd80",
   "metadata": {},
   "source": [
    "- ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ë‹¤ì‹œ trainê³¼ test ì„¸íŠ¸ë¡œ ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pd.isnull(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0f4b6",
   "metadata": {},
   "source": [
    "- train ê²°ì¸¡ì¹˜ ê°œìˆ˜ ìµœì¢… í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72094cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pd.isnull(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f9099",
   "metadata": {},
   "source": [
    "- test ê²°ì¸¡ì¹˜ ê°œìˆ˜ ìµœì¢… í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d25f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---\n",
    "import scipy as sp  # ê³¼í•™ ê³„ì‚°, íŠ¹íˆ ìµœì í™”(optimize)ë¥¼ ìœ„í•´ ì‚¬ìš©\n",
    "\n",
    "from collections import Counter # ë°ì´í„°ì˜ ìš”ì†Œ ê°œìˆ˜ë¥¼ ì„¸ëŠ” ê¸°ëŠ¥\n",
    "from functools import partial   # í•¨ìˆ˜ì˜ ì¸ì ì¤‘ ì¼ë¶€ë¥¼ ê³ ì •í•˜ì—¬ ìƒˆë¡œìš´ í•¨ìˆ˜ë¥¼ ë§Œë“œëŠ” ê¸°ëŠ¥\n",
    "from math import sqrt           # ì œê³±ê·¼ ê³„ì‚°\n",
    "\n",
    "# Scikit-learnì—ì„œ í‰ê°€ ì§€í‘œ(metrics) ê´€ë ¨ í•¨ìˆ˜ë“¤ì„ ì„í¬íŠ¸\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "\n",
    "\n",
    "# --- 2. í‰ê°€ ì§€í‘œ(Quadratic Weighted Kappa) ê³„ì‚° í•¨ìˆ˜ ---\n",
    "# ì›ë³¸ ì¶œì²˜: https://github.com/benhamner/Metrics\n",
    "\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    ë‘ í‰ê°€ì(ì‹¤ì œê°’, ì˜ˆì¸¡ê°’) ê°„ì˜ í˜¼ë™ í–‰ë ¬(Confusion Matrix)ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b)) # ë‘ í‰ê°€ê°’ì˜ ê°œìˆ˜ê°€ ë™ì¼í•œì§€ í™•ì¸\n",
    "    # ìµœì†Œ/ìµœëŒ€ í‰ì (rating)ì´ ì£¼ì–´ì§€ì§€ ì•Šìœ¼ë©´ ë°ì´í„°ì—ì„œ ìë™ìœ¼ë¡œ ì°¾ìŒ\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    # í‰ì ì˜ ì¢…ë¥˜ ê°œìˆ˜ ê³„ì‚° (0, 1, 2, 3, 4 -> 5ê°œ)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    # N x N í¬ê¸°ì˜ 0ìœ¼ë¡œ ì±„ì›Œì§„ í–‰ë ¬ ìƒì„±\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    # ì‹¤ì œê°’(a)ê³¼ ì˜ˆì¸¡ê°’(b)ì„ í•˜ë‚˜ì”© ë¹„êµí•˜ë©° í•´ë‹¹ ìœ„ì¹˜ì˜ ì¹´ìš´íŠ¸ë¥¼ 1ì”© ì¦ê°€\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í‰ì (rating) ë°ì´í„°ì˜ ë¶„í¬(íˆìŠ¤í† ê·¸ë¨)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # ... (confusion_matrixì™€ ìœ ì‚¬í•œ ë¡œì§ìœ¼ë¡œ ê° í‰ì ì´ ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ ì…ˆ)\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    ì´ ëŒ€íšŒì˜ ê³µì‹ í‰ê°€ ì§€í‘œì¸ Quadratic Weighted Kappa ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    ì‹¤ì œê°’(y)ê³¼ ì˜ˆì¸¡ê°’(y_pred)ì´ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. (-1 ~ 1 ì‚¬ì´ì˜ ê°’)\n",
    "    ë‹¨ìˆœíˆ ë§ê³  í‹€ë¦¼ì„ ë„˜ì–´, 'ì–¼ë§ˆë‚˜ ì‹¬í•˜ê²Œ' í‹€ë ¸ëŠ”ì§€ì— ê°€ì¤‘ì¹˜ë¥¼ ë‘ì–´ ì ìˆ˜ë¥¼ ë§¤ê¹ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # ... (ë‚´ë¶€ì ìœ¼ë¡œ ìœ„ì˜ confusion_matrix, histogram í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ê°€ì¤‘ì¹˜ ê³„ì‚° ìˆ˜í–‰)\n",
    "    # ...\n",
    "    return (1.0 - numerator / denominator)\n",
    "\n",
    "# --- 3. ìµœì ì˜ ë¶„ë¥˜ ê²½ê³„ê°’ì„ ì°¾ëŠ” í´ë˜ìŠ¤ ---\n",
    "class OptimizedRounder(object):\n",
    "    \"\"\"\n",
    "    íšŒê·€ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì—°ì†ì ì¸ ê°’(ì˜ˆ: 2.3, 1.8)ì„\n",
    "    ì–´ë–¤ ê²½ê³„ê°’(threshold)ìœ¼ë¡œ ì˜ë¼ì•¼ ê°€ì¥ ë†’ì€ Kappa ì ìˆ˜ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€\n",
    "    ê·¸ ìµœì ì˜ ê²½ê³„ê°’ì„ ì°¾ì•„ì£¼ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0 # ìµœì ì˜ ê²½ê³„ê°’(ê³„ìˆ˜)ì„ ì €ì¥í•  ë³€ìˆ˜\n",
    "\n",
    "    # Kappa ì ìˆ˜ë¥¼ 'ì†ì‹¤ í•¨ìˆ˜'ë¡œ ë³€í™˜í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜ (ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë¯€ë¡œ)\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        # ì£¼ì–´ì§„ ê²½ê³„ê°’(coef)ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ê°’(X_p)ì„ 0, 1, 2, 3, 4ì˜ ë²”ì£¼ë¡œ ë³€í™˜\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            # ... (ì´í•˜ ìƒëµ) ...\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        # ë³€í™˜ëœ ì˜ˆì¸¡ê°’ìœ¼ë¡œ Kappa ì ìˆ˜ë¥¼ ê³„ì‚°\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        # Kappa ì ìˆ˜ëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ, ì•ì— -ë¥¼ ë¶™ì—¬ 'ì†ì‹¤'ë¡œ ë³€í™˜ (ìµœì†Œí™” ë¬¸ì œë¡œ ë§Œë“¤ê¸° ìœ„í•¨)\n",
    "        return -ll\n",
    "\n",
    "    # ìµœì ì˜ ê²½ê³„ê°’ì„ ì°¾ëŠ” ë©”ì¸ í•¨ìˆ˜\n",
    "    def fit(self, X, y):\n",
    "        # ìµœì í™”í•  ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì¤€ë¹„ (Xì™€ y ê°’ì„ ê³ ì •)\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        # ì´ˆê¸° ê²½ê³„ê°’ ì„¤ì •\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        # scipyì˜ ìµœì í™” í•¨ìˆ˜(minimize)ë¥¼ ì‚¬ìš©í•´ ì†ì‹¤(-kappa)ì„ ìµœì†Œí™”í•˜ëŠ” ê²½ê³„ê°’ì„ ì°¾ìŒ\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    # ì°¾ì•„ë‚¸ ìµœì ì˜ ê²½ê³„ê°’ìœ¼ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        # ... (_kappa_lossì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ì˜ˆì¸¡) ...\n",
    "        return X_p\n",
    "\n",
    "    # ìµœì í™” ê²°ê³¼ë¡œ ì°¾ì•„ë‚¸ ê²½ê³„ê°’(ê³„ìˆ˜)ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "\n",
    "# --- 4. RMSE ê³„ì‚° í•¨ìˆ˜ ---\n",
    "def rmse(actual, predicted):\n",
    "    \"\"\"\n",
    "    í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨(Root Mean Squared Error)ë¥¼ ê³„ì‚°í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜.\n",
    "    \"\"\"\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b99dba",
   "metadata": {},
   "source": [
    "- ëŒ€íšŒì˜ ê³µì‹ í‰ê°€ ì§€í‘œì¸ Quadratic Weighted Kappa ì§€í‘œì˜ ì ìˆ˜ê°€ ì˜ ë‚˜ì˜¬ìˆ˜ ìˆë„ë¡ ê³„ì‚° ë¡œì§ì„ ë¯¸ë¦¬ ì„¤ì •\n",
    "\n",
    "-  OptimizedRounder: íšŒê·€ ëª¨ë¸ì„ ìœ„í•œ 'í•„ì‚´ê¸°'\n",
    "\n",
    "ë§ì€ ê²½ìš°, ì…ì–‘ ì†ë„(0, 1, 2, 3, 4)ë¥¼ ì§ì ‘ ì˜ˆì¸¡í•˜ëŠ” ë¶„ë¥˜(Classification) ë¬¸ì œë³´ë‹¤, ì—°ì†ì ì¸ ê°’(ì˜ˆ: 2.7, 3.1)ì„ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€(Regression) ë¬¸ì œë¡œ ì ‘ê·¼í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì´ ë” ì¢‹ì„ ë•Œê°€ ë§ìŠµë‹ˆë‹¤.\n",
    "\n",
    "í•˜ì§€ë§Œ íšŒê·€ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’(2.7, 3.1)ì€ ìµœì¢…ì ìœ¼ë¡œ 0, 1, 2, 3, 4 ì¤‘ í•˜ë‚˜ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë•Œ ë‹¨ìˆœíˆ ë°˜ì˜¬ë¦¼(ì˜ˆ: 2.7 -> 3)í•˜ëŠ” ê²ƒë³´ë‹¤ ë” ì¢‹ì€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "OptimizedRounder í´ë˜ìŠ¤ëŠ” ë°”ë¡œ ì´ **ìµœì ì˜ ë³€í™˜ ê²½ê³„(Threshold)**ë¥¼ ì°¾ì•„ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì‘ë™ ì›ë¦¬: scipy.optimize.minimizeë¼ëŠ” ìµœì í™” ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬, ì–´ë–¤ ê²½ê³„ê°’(ì˜ˆ: [1.7, 2.5, 3.2, 3.8])ìœ¼ë¡œ ì˜ˆì¸¡ê°’ì„ ì˜ë¼ì•¼ Kappa ì ìˆ˜ê°€ ê°€ì¥ ë†’ì•„ì§€ëŠ”ì§€ë¥¼ ìë™ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "íš¨ê³¼: ë‹¨ìˆœíˆ ë°˜ì˜¬ë¦¼í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë” ë†’ì€ ìµœì¢… ì ìˆ˜ë¥¼ ì–»ê²Œ í•´ì£¼ëŠ” ë§¤ìš° ì¤‘ìš”í•œ í›„ì²˜ë¦¬(Post-processing) ê¸°ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "- ì½”ë“œ ë¸”ë¡ ì „ì²´ëŠ” ëª¨ë¸ë§ì˜ í•µì‹¬ ë¡œì§ì´ë¼ê¸°ë³´ë‹¤ëŠ”, ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì •í™•íˆ ì¸¡ì •í•˜ê³  ê·¸ ì„±ëŠ¥ì„ ê·¹í•œê¹Œì§€ ëŒì–´ì˜¬ë¦¬ê¸° ìœ„í•œ ê³ ê¸‰ ìœ í‹¸ë¦¬í‹° ë° ì „ëµì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.85,\n",
    "          'feature_fraction': 0.8,\n",
    "          'min_split_gain': 0.02,\n",
    "          'min_child_samples': 150,\n",
    "          'min_child_weight': 0.02,\n",
    "          'lambda_l2': 0.0475,\n",
    "          'verbosity': -1,\n",
    "          'data_random_seed': 17}\n",
    "\n",
    "# Additional parameters:\n",
    "early_stop = 500\n",
    "verbose_eval = 100\n",
    "num_rounds = 10000\n",
    "n_splits = 5\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_splits, random_state=1337)\n",
    "\n",
    "\n",
    "oof_train = np.zeros((X_train.shape[0]))\n",
    "oof_test = np.zeros((X_test.shape[0], n_splits))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for train_index, valid_index in kfold.split(X_train, X_train['AdoptionSpeed'].values):\n",
    "    \n",
    "    X_tr = X_train.iloc[train_index, :]\n",
    "    X_val = X_train.iloc[valid_index, :]\n",
    "    \n",
    "    y_tr = X_tr['AdoptionSpeed'].values\n",
    "    X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n",
    "    \n",
    "    y_val = X_val['AdoptionSpeed'].values\n",
    "    X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n",
    "    \n",
    "    print('\\ny_tr distribution: {}'.format(Counter(y_tr)))\n",
    "    \n",
    "    d_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    d_valid = lgb.Dataset(X_val, label=y_val)\n",
    "    watchlist = [d_train, d_valid]\n",
    "    \n",
    "    print('training LGB:')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    \n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    oof_train[valid_index] = val_pred\n",
    "    oof_test[:, i] = test_pred\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf92ce68",
   "metadata": {},
   "source": [
    "oof_train, oof_testì˜ ì—­í• \n",
    "oof_train (Out-of-Fold train): ê° ë°ì´í„°ê°€ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©ë˜ì—ˆì„ ë•Œì˜ ì˜ˆì¸¡ê°’ì„ ëª¨ì•„ë†“ì€ ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ í•œ ë²ˆë„ ë³´ì§€ ëª»í•œ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì´ë¯€ë¡œ, ì´ oof_trainê³¼ ì‹¤ì œ ì •ë‹µì„ ë¹„êµí•˜ë©´ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ê°€ì¥ ê°ê´€ì ìœ¼ë¡œ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "oof_test: test ë°ì´í„°ì— ëŒ€í•œ 5ê°œ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ ëª¨ë‘ ì €ì¥í•©ë‹ˆë‹¤. ìµœì¢… ì œì¶œ íŒŒì¼ì„ ë§Œë“¤ ë•Œ ì´ ê°’ë“¤ì˜ í‰ê· ì„ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(oof_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f5df3",
   "metadata": {},
   "source": [
    "1. ì˜ˆì¸¡ê°’ì˜ ì¤‘ì‹¬ ê²½í–¥: ì˜ˆì¸¡ê°’ë“¤ì´ íŠ¹ì • ê°’(ì˜ˆ: 2.5) ì£¼ë³€ì— ë§ì´ ëª°ë ¤ ìˆëŠ”ì§€, ì•„ë‹ˆë©´ ë„“ê²Œ í¼ì ¸ ìˆëŠ”ì§€ë¥¼ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "2. ë¶„í¬ì˜ í˜•íƒœ: ë¶„í¬ê°€ ì •ê·œë¶„í¬ì™€ ë¹„ìŠ·í•œì§€, ì•„ë‹ˆë©´ í•œìª½ìœ¼ë¡œ ì¹˜ìš°ì³ì ¸ ìˆëŠ”ì§€ ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "3. ê²½ê³„ê°’ ì„¤ì •ì˜ ë‹¨ì„œ: ì´ íˆìŠ¤í† ê·¸ë¨ì„ ë³´ê³ , ë‹¤ìŒ ë‹¨ê³„ì¸ OptimizedRounderë¥¼ ì‚¬ìš©í•  ë•Œ ìµœì ì˜ ë¶„ë¥˜ ê²½ê³„ê°’ì´ ëŒ€ëµ ì–´ë””ì¯¤ ìœ„ì¹˜í• ì§€ ì§ê´€ì ì¸ íŒíŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. ìµœì ì˜ ë¶„ë¥˜ ê²½ê³„ê°’(Threshold) ì°¾ê¸° ---\n",
    "# ì´ì „ì— ì •ì˜í•œ OptimizedRounder í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "optR = OptimizedRounder()\n",
    "# OOF ì˜ˆì¸¡ê°’(oof_train)ê³¼ ì‹¤ì œ ì •ë‹µ(AdoptionSpeed)ì„ ì‚¬ìš©í•´ Kappa ì ìˆ˜ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²½ê³„ê°’ì„ ì°¾ìŒ\n",
    "optR.fit(oof_train, X_train['AdoptionSpeed'].values)\n",
    "# ìœ„ì—ì„œ ì°¾ì€ ìµœì ì˜ ê²½ê³„ê°’ì„ 'coefficients' ë³€ìˆ˜ì— ì €ì¥\n",
    "coefficients = optR.coefficients()\n",
    "\n",
    "# --- 2. ìµœì  ê²½ê³„ê°’ì„ ì ìš©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ ìƒì„± ---\n",
    "# ì°¾ì€ ê²½ê³„ê°’(coefficients)ì„ oof_train ì˜ˆì¸¡ê°’ì— ì ìš©í•˜ì—¬ ìµœì¢… ë¶„ë¥˜(0~4) ê²°ê³¼ë¥¼ ìƒì„±\n",
    "pred_test_y_k = optR.predict(oof_train, coefficients)\n",
    "\n",
    "# --- 3. ê²°ê³¼ í™•ì¸ ë° ìµœì¢… ì ìˆ˜ ê³„ì‚° ---\n",
    "# ì‹¤ì œ ì •ë‹µê°’ë“¤ì˜ ë¶„í¬ë¥¼ ì¶œë ¥\n",
    "print(\"\\nValid Counts = \", Counter(X_train['AdoptionSpeed'].values))\n",
    "# ìµœì í™”ëœ ì˜ˆì¸¡ê°’ë“¤ì˜ ë¶„í¬ë¥¼ ì¶œë ¥\n",
    "print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "# ì°¾ì•„ë‚¸ ìµœì ì˜ ê²½ê³„ê°’ë“¤ì„ ì¶œë ¥\n",
    "print(\"Coefficients = \", coefficients)\n",
    "# ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ì •ë‹µ ì‚¬ì´ì˜ Quadratic Weighted Kappa ì ìˆ˜ë¥¼ ê³„ì‚°\n",
    "qwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, pred_test_y_k)\n",
    "# ìµœì¢… QWK ì ìˆ˜ë¥¼ ì¶œë ¥\n",
    "print(\"QWK = \", qwk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34dfe2b",
   "metadata": {},
   "source": [
    "ë‹¨ìˆœ ë°˜ì˜¬ë¦¼ ëŒ€ì‹  ë°ì´í„°ì— ìµœì í™”ëœ ê¸°ì¤€ì„ ì°¾ì•„ íšŒê·€ ì˜ˆì¸¡ì„ ë¶„ë¥˜ë¡œ ë³€í™˜í•¨ìœ¼ë¡œì¨, ëª¨ë¸ì˜ ì ì¬ ì„±ëŠ¥ì„ ìµœëŒ€í•œìœ¼ë¡œ ëŒì–´ë‚´ëŠ” ë§¤ìš° ì¤‘ìš”í•œ í›„ì²˜ë¦¬ ê³¼ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually adjusted coefficients:\n",
    "\n",
    "coefficients_ = coefficients.copy()\n",
    "\n",
    "coefficients_[0] = 1.645 # ì²« ë²ˆì§¸ ê²½ê³„ì„ (0ê³¼ 1ì„ ë‚˜ëˆ„ëŠ” ì„ )ì˜ ìœ„ì¹˜ë¥¼ 1.645ë¡œ ì˜®ê¸´ë‹¤.\n",
    "coefficients_[1] = 2.115 # ë‘ ë²ˆì§¸ ê²½ê³„ì„ (1ê³¼ 2ë¥¼ ë‚˜ëˆ„ëŠ” ì„ )ì˜ ìœ„ì¹˜ë¥¼ 2.115ë¡œ ì˜®ê¸´ë‹¤.\n",
    "coefficients_[3] = 2.84 # ì„¸ ë²ˆì§¸ ê²½ê³„ì„ (3ê³¼ 4ë¥¼ ë‚˜ëˆ„ëŠ” ì„ )ì˜ ìœ„ì¹˜ë¥¼ 2.84ë¡œ ì˜®ê¸´ë‹¤.\n",
    "\n",
    "train_predictions = optR.predict(oof_train, coefficients_).astype(int)\n",
    "print('train pred distribution: {}'.format(Counter(train_predictions)))\n",
    "\n",
    "test_predictions = optR.predict(oof_test.mean(axis=1), coefficients_)\n",
    "print('test pred distribution: {}'.format(Counter(test_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c949fa4c",
   "metadata": {},
   "source": [
    "```\n",
    "<-- ë“±ê¸‰ 0 --|-- ë“±ê¸‰ 1 --|-- ë“±ê¸‰ 2 --|-- ë“±ê¸‰ 3 --|-- ë“±ê¸‰ 4 -->\n",
    "             â†‘           â†‘            â†‘            â†‘\n",
    "        ê²½ê³„ì„  1      ê²½ê³„ì„  2     ê²½ê³„ì„  3      ê²½ê³„ì„  4\n",
    "   (coefficients[0]) (coefficients[1]) (coefficients[2]) (coefficients[3])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00d20d",
   "metadata": {},
   "source": [
    "- ì„ì˜ë¡œ ìµœì ê°’ ìˆ˜ì •\n",
    "\n",
    "- êµ­ì†Œ ìµœì í•´ (Local Minimum): ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ ì™„ë²½í•œ ì „ì—­ ìµœì í•´(Global Minimum)ê°€ ì•„ë‹Œ, ê·¸ëŸ´ë“¯í•œ êµ­ì†Œ ìµœì í•´ì— ë¨¸ë¬¼ë €ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- ë°ì´í„°ì˜ ë¯¸ë¬˜í•œ íŠ¹ì„±: OOF ì˜ˆì¸¡ê°’ì˜ ë¶„í¬ë‚˜ ì‹¤ì œ ì •ë‹µì˜ ë¶„í¬ë¥¼ ì‹œê°í™”í•´ ë³¸ ë¶„ì„ê°€ê°€ \"ê²½ê³„ê°’ì„ ì•½ê°„ë§Œ ì˜®ê¸°ë©´ ë” ì•ˆì •ì ì¸ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•˜ê² ë‹¤\"ê³  íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ê²½ê³„ê°’ ê·¼ì²˜ì— ì˜ˆì¸¡ê°’ë“¤ì´ ë„ˆë¬´ ë¹½ë¹½í•˜ê²Œ ëª¨ì—¬ ìˆë‹¤ë©´, ì•½ê°„ì˜ ì¡°ì •ìœ¼ë¡œ ë¶„ë¥˜ ê²°ê³¼ê°€ ë” ì•ˆì •ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd3e64",
   "metadata": {},
   "source": [
    "êµ­ì†Œ ìµœì í•´ëŠ” ì£¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì— ë°œìƒí•©ë‹ˆë‹¤.\n",
    "\n",
    "1. ë¬¸ì œ ìì²´ê°€ ë³µì¡í•˜ê³  ìš¸í‰ë¶ˆí‰í•  ë•Œ (Non-convex Problems)\n",
    "ê°€ì¥ ê·¼ë³¸ì ì¸ ì›ì¸ì…ë‹ˆë‹¤. í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œì˜ ì •ë‹µì„ ì°¾ëŠ” ê³¼ì •ì´ í‰íƒ„í•œ ì–¸ë•ì´ ì•„ë‹ˆë¼, ì—¬ëŸ¬ ê°œì˜ ë´‰ìš°ë¦¬ì™€ ê³„ê³¡ì´ ìˆëŠ” í—˜ì¤€í•œ ì‚°ë§¥ì„ íƒí—˜í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ê²½ìš°ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë¹„ìœ : ê°€ì¥ ë†’ì€ ì‚°ë´‰ìš°ë¦¬(ì „ì²´ ìµœì í•´)ë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©í‘œì¸ë°, ì•ˆê°œê°€ ììš±í•´ì„œ ì£¼ë³€ë§Œ ë³´ì…ë‹ˆë‹¤. ì¼ë‹¨ ê°€ì¥ ë†’ì€ ê³³ìœ¼ë¡œ ê³„ì† ì˜¬ë¼ê°€ë‹¤ ë³´ë‹ˆ ì–´ë–¤ ë´‰ìš°ë¦¬(êµ­ì†Œ ìµœì í•´)ì— ë„ì°©í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì•ˆê°œ ë•Œë¬¸ì— ì € ë©€ë¦¬ ë” ë†’ì€ ì§„ì§œ ì •ìƒ(ì „ì²´ ìµœì í•´)ì´ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œì§€ ëª»í•˜ê³  íƒí—˜ì„ ë©ˆì¶”ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ì œ ì˜ˆì‹œ: ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •, ë³µì¡í•œ ì‹œìŠ¤í…œì˜ ìµœì í™” ë¬¸ì œ ë“± ìˆ˜ë§ì€ ë³€ìˆ˜ë“¤ì´ ì„œë¡œ ì–½í˜€ìˆëŠ” ë¬¸ì œë“¤ì´ ì—¬ê¸°ì— í•´ë‹¹í•©ë‹ˆë‹¤.\n",
    "\n",
    "2. ì•Œê³ ë¦¬ì¦˜ì´ 'íƒìš•ì (Greedy)'ì¼ ë•Œ\n",
    "ì•Œê³ ë¦¬ì¦˜ì´ ì „ì²´ì ì¸ í° ê·¸ë¦¼ì„ ë³´ì§€ ì•Šê³ , ë§¤ ìˆœê°„ ëˆˆì•ì˜ ì´ìµì´ ê°€ì¥ í° ë°©í–¥ìœ¼ë¡œë§Œ ì›€ì§ì´ëŠ” ë°©ì‹ì¼ ë•Œ êµ­ì†Œ ìµœì í•´ì— ë¹ ì§€ê¸° ì‰½ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë¹„ìœ : ë™ë„¤ì—ì„œ ê°€ì¥ ë†’ì€ ê³³ìœ¼ë¡œ ê°€ë ¤ê³  í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ê¸¸ì€ ë³´ì§€ ì•Šê³ , ë¬´ì¡°ê±´ í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê°€ì¥ ê²½ì‚¬ê°€ ê°€íŒŒë¥¸ ì˜¤ë¥´ë§‰ê¸¸ë¡œë§Œ ê³„ì† ì˜¬ë¼ê°‘ë‹ˆë‹¤. ê·¸ëŸ¬ë‹¤ ê²°êµ­ ë™ë„¤ ë’·ì‚° ì •ìƒì—ëŠ” ë„ì°©í–ˆì§€ë§Œ, ë„ì‹œ ì „ì²´ì—ì„œ ê°€ì¥ ë†’ì€ ë‚¨ì‚°íƒ€ì›Œë¡œ ê°€ëŠ” ê¸¸ì€ ë†“ì¹˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ì œ ì˜ˆì‹œ: í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆì‹œì²˜ëŸ¼ \"ì¼ë‹¨ ë² íƒ€ ë‚®ì€ ê²ƒë§Œ ê³ ë¥´ê³ , ê·¸ ì•ˆì—ì„œ ìˆ˜ìµë¥  ì œì¼ ë†’ì€ ê²ƒ\"ê³¼ ê°™ì´ ë‹¨ê³„ë³„ë¡œ ìµœì„ ì„ ì„ íƒí•˜ëŠ” ë°©ì‹, ì¼ë¶€ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ë“±ì´ ì—¬ê¸°ì— í•´ë‹¹í•©ë‹ˆë‹¤.\n",
    "\n",
    "3. ì‹œì‘ ì§€ì ì´ ì¢‹ì§€ ì•Šì„ ë•Œ\n",
    "ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì€ ë³´í†µ ì„ì˜ì˜ ì§€ì ì—ì„œ íƒìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì´ë•Œ ì–´ë””ì„œ ì‹œì‘í–ˆëŠ”ì§€ì— ë”°ë¼ ìµœì¢…ì ìœ¼ë¡œ ë„ë‹¬í•˜ëŠ” ë´‰ìš°ë¦¬ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë¹„ìœ : í—¬ë¦¬ì½¥í„°ê°€ ë‘ ëª…ì˜ íƒí—˜ê°€ë¥¼ ì‚°ë§¥ì˜ ì„œë¡œ ë‹¤ë¥¸ ì§€ì ì— ë‚´ë ¤ì£¼ì—ˆìŠµë‹ˆë‹¤. ë‘ ì‚¬ëŒ ëª¨ë‘ ê°ìì˜ ìœ„ì¹˜ì—ì„œ ê°€ì¥ ë†’ì€ ë´‰ìš°ë¦¬ë¥¼ í–¥í•´ ì˜¬ë¼ê°”ì§€ë§Œ, ì‹œì‘ ìœ„ì¹˜ê°€ ë‹¬ëê¸° ë•Œë¬¸ì— ê²°êµ­ ì„œë¡œ ë‹¤ë¥¸ ë´‰ìš°ë¦¬ì— ë„ë‹¬í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ì œ ì˜ˆì‹œ: K-í‰ê·  í´ëŸ¬ìŠ¤í„°ë§(K-Means Clustering)ì€ ì²˜ìŒì— ì¤‘ì‹¬ì ì„ ì–´ë””ì— ì°ëŠëƒì— ë”°ë¼ ìµœì¢… í´ëŸ¬ìŠ¤í„° ê²°ê³¼ê°€ í¬ê²Œ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ë²ˆ ë‹¤ë¥¸ ì‹œì‘ì ì—ì„œ ì‹¤í–‰í•´ë³´ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "4. ê·œì¹™(ëª¨ë¸)ì´ ë„ˆë¬´ ë‹¨ìˆœí•  ë•Œ (ì‚¬ìš©ìê»˜ì„œ ì§€ì í•˜ì‹  ê²½ìš°)\n",
    "ë¬¸ì œì˜ ë³µì¡ì„±ì„ ì¶©ë¶„íˆ ë‹´ì•„ë‚´ì§€ ëª»í•˜ëŠ” ì§€ë‚˜ì¹˜ê²Œ ë‹¨ìˆœí•œ ëª¨ë¸ì´ë‚˜ ê·œì¹™ì„ ì‚¬ìš©í•˜ë©´, ë‹¤ì–‘í•œ ê°€ëŠ¥ì„±ì„ íƒìƒ‰í•˜ì§€ ëª»í•˜ê³  ë»”í•œ ê²°ë¡ ì— ë„ë‹¬í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ë¹„ìœ : \"ê°€ì¥ ëˆì„ ë§ì´ ë²„ëŠ” ë°©ë²•ì€?\"ì´ë¼ëŠ” ì§ˆë¬¸ì— \"ê°€ì¥ ì›”ê¸‰ì„ ë§ì´ ì£¼ëŠ” íšŒì‚¬ í•œ êµ°ë°ì— ì·¨ì§í•˜ëŠ” ê²ƒ\"ì´ë¼ê³  ë‹µí•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ì‚¬ì—…, íˆ¬ì, ë¶€ì—… ë“± ë‹¤ì–‘í•œ ì¡°í•©ì„ í†µí•œ ìµœì ì˜ ë¶€ ì°½ì¶œ ë°©ë²•ì„ ê³ ë ¤í•˜ì§€ ëª»í•œ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ì œ ì˜ˆì‹œ: ì‚¬ìš©ìê»˜ì„œ ê²½í—˜í•˜ì‹  í¬íŠ¸í´ë¦¬ì˜¤ 'ëª°ë¹µ' ì¶”ì²œì´ ë°”ë¡œ ì´ ê²½ìš°ì— í•´ë‹¹í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ìì‚°ì„ ì¡°í•©í–ˆì„ ë•Œì˜ ìœ„í—˜ ë¶„ì‚° íš¨ê³¼ë¼ëŠ” ì¤‘ìš”í•œ ê·œì¹™ì„ ëª¨ë¸ì´ ê³ ë ¤í•˜ì§€ ì•Šì€ ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution inspection of original target and predicted train and test:\n",
    "\n",
    "print(\"True Distribution:\")\n",
    "print(pd.value_counts(X_train['AdoptionSpeed'], normalize=True).sort_index())\n",
    "print(\"\\nTrain Predicted Distribution:\")\n",
    "print(pd.value_counts(train_predictions, normalize=True).sort_index())\n",
    "print(\"\\nTest Predicted Distribution:\")\n",
    "print(pd.value_counts(test_predictions, normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a84c19b",
   "metadata": {},
   "source": [
    "- ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê²°ê³¼ì™€ ì‹¤ì œ ë°ì´í„°ì˜ ë¶„í¬ ì¡°ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113381b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission:\n",
    "\n",
    "submission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions.astype(np.int32)})\n",
    "submission.head()\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
