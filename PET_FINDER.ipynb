{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 기본 및 데이터 처리 라이브러리 ---\n",
    "import gc  # Garbage Collector: 메모리 관리를 위한 라이브러리\n",
    "import glob  # 파일 경로명을 이용해 원하는 파일들을 찾아주는 라이브러리\n",
    "import os  # 운영체제와 상호작용하기 위한 라이브러리 (파일 경로 등)\n",
    "import json  # JSON 형식의 파일을 읽고 쓰기 위한 라이브러리\n",
    "import pprint  # 데이터 구조를 예쁘게 출력하기 위한 라이브러리\n",
    "\n",
    "# --- 시각화 라이브러리 ---\n",
    "import matplotlib.pyplot as plt  # 데이터 시각화를 위한 핵심 라이브러리\n",
    "\n",
    "# --- 수치 계산 및 데이터 분석 라이브러리 ---\n",
    "import numpy as np  # 다차원 배열 및 고성능 수치 계산을 위한 라이브러리\n",
    "import pandas as pd  # 데이터프레임(표 형태의 데이터)을 다루기 위한 핵심 라이브러리\n",
    "\n",
    "# --- 병렬 처리 및 유틸리티 라이브러리 ---\n",
    "from joblib import Parallel, delayed  # 작업을 병렬로 처리하여 속도를 높이기 위한 라이브러리\n",
    "from tqdm import tqdm  # 반복문의 진행 상태를 시각적인 막대로 보여주는 라이브러리\n",
    "from PIL import Image  # 이미지 파일을 열고 다루기 위한 라이브러리 (Pillow)\n",
    "\n",
    "# --- Jupyter Notebook 설정 ---\n",
    "# matplotlib으로 그린 그래프를 노트북 셀 바로 아래에 표시하도록 설정하는 매직 명령어\n",
    "%matplotlib inline\n",
    "\n",
    "# --- Pandas 출력 옵션 설정 ---\n",
    "# 데이터프레임을 출력할 때 보여줄 최대 행과 열의 개수를 설정\n",
    "pd.options.display.max_rows = 128\n",
    "pd.options.display.max_columns = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa48d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 시각화 그래프의 기본 크기를 지정 ---\n",
    "plt.rcParams['figure.figsize'] = (12, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59822c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 데이터 디렉터리의 내용을 확인 ---\n",
    "os.listdir('../input/test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c51b7f",
   "metadata": {},
   "source": [
    "- '../input/test/'\n",
    "  - Kaggle 노트북 환경에서 모든 대회 데이터는 ../input/ 폴더 아래에 위치합니다. 따라서 이 경로는 대회의 test 데이터셋 폴더를 가리킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train/train.csv')\n",
    "test = pd.read_csv('../input/test/test.csv')\n",
    "sample_submission = pd.read_csv('../input/test/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedfe7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_breed = pd.read_csv('../input/breed_labels.csv')\n",
    "labels_state = pd.read_csv('../input/state_labels.csv')\n",
    "labels_color = pd.read_csv('../input/color_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_files = sorted(glob.glob('../input/train_images/*.jpg'))\n",
    "train_metadata_files = sorted(glob.glob('../input/train_metadata/*.json'))\n",
    "train_sentiment_files = sorted(glob.glob('../input/train_sentiment/*.json'))\n",
    "\n",
    "print('num of train images files: {}'.format(len(train_image_files)))\n",
    "print('num of train metadata files: {}'.format(len(train_metadata_files)))\n",
    "print('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n",
    "\n",
    "\n",
    "test_image_files = sorted(glob.glob('../input/test_images/*.jpg'))\n",
    "test_metadata_files = sorted(glob.glob('../input/test_metadata/*.json'))\n",
    "test_sentiment_files = sorted(glob.glob('../input/test_sentiment/*.json'))\n",
    "\n",
    "print('num of test images files: {}'.format(len(test_image_files)))\n",
    "print('num of test metadata files: {}'.format(len(test_metadata_files)))\n",
    "print('num of test sentiment files: {}'.format(len(test_sentiment_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b728e",
   "metadata": {},
   "source": [
    "CSV 파일은 그 자체로 **\"하나의 완성된 표\"**이고, 비정형 데이터는 **\"수많은 개별 재료의 목록\"**이기 때문에 비정형파일만 파일의 개수를 확인\n",
    "\n",
    "## 비정형 데이터 파일 (.jpg, .json): \"수많은 재료의 목록\"을 생성\n",
    "- 처리 방식: glob.glob()은 수천 개에 달하는 개별 파일의 내용을 읽는 것이 아니라, 파일들의 경로(위치)만 찾아서 리스트(list)로 만듭니다. 아직은 어떤 정보도 처리하지 않은, 말 그대로 \"재료 목록\"만 만든 상태입니다.\n",
    "\n",
    "- 확인 대상: 이 단계에서 가장 중요한 정보는 \"그래서 파일이 총 몇 개나 있는가?\" 입니다. 이 개수를 통해 데이터의 전체 규모를 파악하고, train.csv 파일의 행 개수와 비교하며 데이터가 누락되지 않았는지 1차적으로 점검할 수 있습니다.\n",
    "\n",
    "- 점검 방법: 따라서 리스트에 들어있는 경로가 총 몇 개인지 len() 함수를 사용해 개수를 세어 출력하는 것이 가장 효과적인 첫 번째 점검 방법입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd94654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib 그래프의 시각적 스타일을 'ggplot'으로 설정 (R언어에서 유래한 인기 스타일)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# --- 1. 이미지(Images) 데이터 커버리지 확인 ---\n",
    "# 원본 train 데이터에서 'PetID' 컬럼만 추출하여 기준이 될 ID 목록 생성\n",
    "train_df_ids = train[['PetID']]\n",
    "# 원본 train 데이터의 전체 반려동물 수를 출력\n",
    "print(train_df_ids.shape)\n",
    "\n",
    "# 이전에 수집한 이미지 파일 경로 리스트를 데이터프레임으로 변환\n",
    "train_df_imgs = pd.DataFrame(train_image_files)\n",
    "train_df_imgs.columns = ['image_filename'] # 컬럼 이름 지정\n",
    "# 파일명에서 PetID를 추출. 예: '../.../a1b2c3d4-1.jpg' -> 'a1b2c3d4'\n",
    "train_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "# 추출한 PetID를 'PetID'라는 새 컬럼으로 추가\n",
    "train_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\n",
    "# 이미지가 있는 반려동물의 고유한 ID 개수를 출력\n",
    "print(len(train_imgs_pets.unique()))\n",
    "\n",
    "# 이미지 파일의 PetID와 원본 train 데이터의 PetID 사이의 교집합(공통 ID)을 구함\n",
    "pets_with_images = len(np.intersect1d(train_imgs_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "# 원본 데이터의 모든 반려동물 중, 이미지가 있는 동물의 비율을 계산하여 출력\n",
    "print('fraction of pets with images: {:.3f}'.format(pets_with_images / train_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# --- 2. 메타데이터(Metadata) 커버리지 확인 ---\n",
    "# (위 이미지 데이터 처리와 과정이 거의 동일)\n",
    "train_df_ids = train[['PetID']]\n",
    "train_df_metadata = pd.DataFrame(train_metadata_files)\n",
    "train_df_metadata.columns = ['metadata_filename']\n",
    "# 파일명에서 PetID를 추출. 예: '../.../a1b2c3d4-1.json' -> 'a1b2c3d4'\n",
    "train_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "train_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\n",
    "print(len(train_metadata_pets.unique()))\n",
    "\n",
    "pets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "# 원본 데이터의 모든 반려동물 중, 메타데이터가 있는 동물의 비율을 계산하여 출력\n",
    "print('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / train_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# --- 3. 감성 분석(Sentiment) 데이터 커버리지 확인 ---\n",
    "# (위 데이터 처리와 과정이 거의 동일하나, PetID 추출 방식에 차이가 있음)\n",
    "train_df_ids = train[['PetID']]\n",
    "train_df_sentiment = pd.DataFrame(train_sentiment_files)\n",
    "train_df_sentiment.columns = ['sentiment_filename']\n",
    "# 파일명에서 PetID를 추출. 예: '../.../a1b2c3d4.json' -> 'a1b2c3d4'\n",
    "# 여기서는 파일명에 '-'가 없으므로 '.'을 기준으로 분리\n",
    "train_sentiment_pets = train_df_sentiment['filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "train_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\n",
    "print(len(train_sentiment_pets.unique()))\n",
    "\n",
    "pets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "# 원본 데이터의 모든 반려동물 중, 감성 분석 데이터가 있는 동물의 비율을 계산하여 출력\n",
    "print('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / train_df_ids.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081b887",
   "metadata": {},
   "source": [
    "앞에서 보았듯이, 비정형 데이터 파일의 내부에는 어떤 내용이 있는지 바로 파악할 수 없다. <br>\n",
    "따라서 \"전체 동물 중 몇 퍼센트가 각 데이터를 가지고 있는가?\" 즉, 데이터의 이용 가능성을 파악한다.<br>\n",
    "\n",
    "이 과정을 통해 각 비정형 데이터가 얼마나 유용한지, 데이터가 부족한 경우 어떻게 처리할지(예: 특정 데이터는 사용하지 않거나, 없는 값을 채워 넣는 등)에 대한 분석 전략을 세울 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 테스트 세트(Test Set)의 데이터 커버리지 확인 ---\n",
    "# 아래 3개의 섹션(Images, Metadata, Sentiment)은\n",
    "# 이전에 Train 세트에 대해 수행했던 것과 완전히 동일한 과정입니다.\n",
    "# 단지 대상이 test 데이터라는 점만 다릅니다.\n",
    "\n",
    "# Images:\n",
    "test_df_ids = test[['PetID']]\n",
    "print(test_df_ids.shape)\n",
    "\n",
    "test_df_imgs = pd.DataFrame(test_image_files)\n",
    "test_df_imgs.columns = ['image_filename']\n",
    "test_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "test_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n",
    "print(len(test_imgs_pets.unique()))\n",
    "\n",
    "pets_with_images = len(np.intersect1d(test_imgs_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "# 테스트 세트의 모든 반려동물 중, 이미지가 있는 동물의 비율을 계산\n",
    "print('fraction of pets with images: {:.3f}'.format(pets_with_images / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# Metadata:\n",
    "test_df_ids = test[['PetID']]\n",
    "test_df_metadata = pd.DataFrame(test_metadata_files)\n",
    "test_df_metadata.columns = ['metadata_filename']\n",
    "test_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "test_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\n",
    "print(len(test_metadata_pets.unique()))\n",
    "\n",
    "pets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "# 테스트 세트의 모든 반려동물 중, 메타데이터가 있는 동물의 비율을 계산\n",
    "print('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# Sentiment:\n",
    "test_df_ids = test[['PetID']]\n",
    "test_df_sentiment = pd.DataFrame(test_sentiment_files)\n",
    "test_df_sentiment.columns = ['sentiment_filename']\n",
    "test_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "test_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\n",
    "print(len(test_sentiment_pets.unique()))\n",
    "\n",
    "pets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "# 테스트 세트의 모든 반려동물 중, 감성 분석 데이터가 있는 동물의 비율을 계산\n",
    "print('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# --- 2. 데이터 일관성 최종 확인 ---\n",
    "# 이미지 파일에서 추출한 PetID 목록과 메타데이터 파일에서 추출한 PetID 목록이\n",
    "# 순서까지 완전히 동일한지 확인합니다.\n",
    "print('images and metadata distributions the same? {}'.format(\n",
    "    np.all(test_metadata_pets == test_imgs_pets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4515f",
   "metadata": {},
   "source": [
    "위에서 수행한 작업이 train 셋에 대한 작업이었다면, 이 작업은 test 셋에 대한 작업니다.\n",
    "\n",
    "마지막 한 줄의 코드는 데이터의 일관성을 검사하는 코드이다.\n",
    "코드의 의미는\n",
    "```\n",
    "test_metadata_pets == test_imgs_pets: metadata 파일에서 추출한 PetID 목록과 image 파일에서 추출한 PetID 목록을 항목별로 하나하나 비교합니다. 두 목록의 길이가 같고, 같은 위치에 있는 PetID가 서로 동일하면 True, 하나라도 다르면 False를 반환합니다.\n",
    "\n",
    "np.all(): 위 비교 결과가 모두 True일 때만 최종적으로 True를 반환합니다.\n",
    "```\n",
    "만약 이 결과가 True라면, 이는 **\"메타데이터가 있는 모든 동물은 이미지도 가지고 있으며, 그 순서까지 완벽하게 일치한다\"**는 것을 의미합니다. 이는 데이터가 매우 깨끗하고 정리가 잘 되어있다는 긍정적인 신호   \n",
    "\n",
    "\n",
    "실제 Output은 TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비정형 데이터(JSON, 이미지)를 처리하고 분석 가능한 특징(feature)으로 변환하기 위한 클래스\n",
    "class PetFinderParser(object):\n",
    "    \n",
    "    # 클래스가 생성될 때 초기 설정을 담당하는 생성자 함수\n",
    "    def __init__(self, debug=False):\n",
    "        \n",
    "        self.debug = debug  # 디버그 모드 여부 (현재 코드에서는 사용되지 않음)\n",
    "        self.sentence_sep = ' '  # 문장이나 단어를 합칠 때 사용할 구분자 (공백)\n",
    "        \n",
    "        # 주석: 메인 데이터프레임에 이미 'description'이 있으므로, sentiment 파일에서\n",
    "        # 텍스트를 중복으로 추출할 필요가 없다는 의미.\n",
    "        self.extract_sentiment_text = False # 감성 분석 파일에서 원문 텍스트를 추출할지 여부\n",
    "        \n",
    "        \n",
    "    # --- 파일 로드 함수들 ---\n",
    "    \n",
    "    def open_metadata_file(self, filename):\n",
    "        \"\"\"\n",
    "        메타데이터 JSON 파일을 열고 내용을 읽어옵니다.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as f: # 파일을 읽기 모드('r')로 열기\n",
    "            metadata_file = json.load(f) # JSON 파일의 내용을 파이썬 딕셔너리로 변환\n",
    "        return metadata_file\n",
    "            \n",
    "    def open_sentiment_file(self, filename):\n",
    "        \"\"\"\n",
    "        감성 분석 JSON 파일을 열고 내용을 읽어옵니다.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as f: # 파일을 읽기 모드('r')로 열기\n",
    "            sentiment_file = json.load(f) # JSON 파일의 내용을 파이썬 딕셔너리로 변환\n",
    "        return sentiment_file\n",
    "            \n",
    "    def open_image_file(self, filename):\n",
    "        \"\"\"\n",
    "        이미지 파일을 열고 numpy 배열로 변환합니다. (이 노트북에서는 직접 사용되지 않음)\n",
    "        \"\"\"\n",
    "        image = np.asarray(Image.open(filename)) # 이미지를 숫자 배열로 변환\n",
    "        return image\n",
    "        \n",
    "    # --- 파일 내용 파싱(Parsing) 및 특징 추출 함수들 ---\n",
    "        \n",
    "    def parse_sentiment_file(self, file):\n",
    "        \"\"\"\n",
    "        감성 분석 파일(딕셔너리)을 입력받아, 감성 특징이 담긴 데이터프레임을 반환합니다.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 문서 전체의 감성 점수(magnitude, score)를 추출\n",
    "        file_sentiment = file['documentSentiment']\n",
    "        # 텍스트에서 인식된 주요 개체(고양이, 장난감 등)의 이름을 리스트로 추출\n",
    "        file_entities = [x['name'] for x in file['entities']]\n",
    "        # 개체 이름들을 공백으로 구분된 하나의 문자열로 합침\n",
    "        file_entities = self.sentence_sep.join(file_entities)\n",
    "\n",
    "        # self.extract_sentiment_text가 True일 때만 실행되는 블록 (현재는 False)\n",
    "        if self.extract_sentiment_text:\n",
    "            # 모든 문장의 원문 텍스트를 추출하여 하나의 문자열로 합침\n",
    "            file_sentences_text = [x['text']['content'] for x in file['sentences']]\n",
    "            file_sentences_text = self.sentence_sep.join(file_sentences_text)\n",
    "            # 모든 문장 각각의 감성 점수를 추출\n",
    "            file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n",
    "            \n",
    "            # 문장별 감성 점수들을 데이터프레임으로 만들고, 각 점수(magnitude, score)의 합계를 구함\n",
    "            file_sentences_sentiment = pd.DataFrame.from_dict(\n",
    "                file_sentences_sentiment, orient='columns').sum()\n",
    "            # 계산된 점수 컬럼명에 'document_' 접두사를 추가하여 딕셔너리로 변환\n",
    "            file_sentences_sentiment = file_sentences_sentiment.add_prefix('document_').to_dict()\n",
    "            \n",
    "            # 문서 전체 감성 점수 딕셔너리에 문장별 합산 점수를 추가\n",
    "            file_sentiment.update(file_sentences_sentiment)\n",
    "        \n",
    "        # 최종 감성 점수 딕셔너리를 데이터프레임으로 변환\n",
    "        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n",
    "        if self.extract_sentiment_text:\n",
    "            # 텍스트 추출이 True였다면, 'text' 컬럼을 추가\n",
    "            df_sentiment['text'] = file_sentences_text\n",
    "            \n",
    "        # 개체(entities) 정보를 'entities' 컬럼으로 추가\n",
    "        df_sentiment['entities'] = file_entities\n",
    "        # 모든 컬럼명에 'sentiment_' 접두사를 붙여 다른 데이터와 구분\n",
    "        df_sentiment = df_sentiment.add_prefix('sentiment_')\n",
    "        \n",
    "        return df_sentiment\n",
    "    \n",
    "    def parse_metadata_file(self, file):\n",
    "        \"\"\"\n",
    "        메타데이터 파일(딕셔너리)을 입력받아, 이미지 특징이 담긴 데이터프레임을 반환합니다.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 파일에 어떤 최상위 키들이 있는지 확인\n",
    "        file_keys = list(file.keys())\n",
    "        \n",
    "        # 'labelAnnotations' 키가 있는지 확인 (이미지 라벨 정보)\n",
    "        if 'labelAnnotations' in file_keys:\n",
    "            # 인식된 라벨 중 상위 30%만 사용 (정보가 너무 많은 것을 방지)\n",
    "            file_annots = file['labelAnnotations'][:int(len(file['labelAnnotations']) * 0.3)]\n",
    "            # 상위 라벨들의 신뢰도 점수(score)의 평균을 계산\n",
    "            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n",
    "            # 상위 라벨들의 설명(description)을 리스트로 저장\n",
    "            file_top_desc = [x['description'] for x in file_annots]\n",
    "        else:\n",
    "            # 라벨 정보가 없는 경우, 점수는 NaN(결측치)으로, 설명은 빈칸으로 처리\n",
    "            file_top_score = np.nan\n",
    "            file_top_desc = ['']\n",
    "        \n",
    "        # 이미지의 지배적인 색상 정보와 구도(crop) 정보를 추출\n",
    "        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "        file_crops = file['cropHintsAnnotation']['cropHints']\n",
    "\n",
    "        # 각 색상의 신뢰도 점수와 픽셀 비율의 평균을 계산\n",
    "        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "\n",
    "        # 구도 추천 점수(confidence)의 평균을 계산\n",
    "        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n",
    "        \n",
    "        # 구도 정보에 'importanceFraction' 키가 있는지 확인 (없는 경우도 있음)\n",
    "        if 'importanceFraction' in file_crops[0].keys():\n",
    "            # 중요도(importance) 점수의 평균을 계산\n",
    "            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n",
    "        else:\n",
    "            # 없는 경우, NaN으로 처리\n",
    "            file_crop_importance = np.nan\n",
    "\n",
    "        # 위에서 추출하고 계산한 특징들을 하나의 딕셔너리로 정리\n",
    "        df_metadata = {\n",
    "            'annots_score': file_top_score,\n",
    "            'color_score': file_color_score,\n",
    "            'color_pixelfrac': file_color_pixelfrac,\n",
    "            'crop_conf': file_crop_conf,\n",
    "            'crop_importance': file_crop_importance,\n",
    "            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n",
    "        }\n",
    "        \n",
    "        # 정리된 딕셔너리를 데이터프레임으로 변환\n",
    "        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n",
    "        # 모든 컬럼명에 'metadata_' 접두사를 붙여 다른 데이터와 구분\n",
    "        df_metadata = df_metadata.add_prefix('metadata_')\n",
    "        \n",
    "        return df_metadata\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cb15c5",
   "metadata": {},
   "source": [
    "🔎 파싱(Parsing): 문자열이나 파일, JSON 등의 복잡한 데이터를 일정한 규칙으로 분석하고, 구조화하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa604d",
   "metadata": {},
   "source": [
    "1. 복잡한 딕셔너리 형태의 JSON 파일 내용을 입력받아, 그 안에서 의미 있는 정보(감성 점수, 이미지 라벨, 색상 정보 등)만 골라내고 계산(평균 등)하여, 최종적으로는 분석하기 쉬운 한 줄짜리 데이터프레임으로 만들어 반환   \n",
    "파이썬 코드가 직접 감성 분석을 수행하는 것은 아니고, 그냥 json 파일을 어떻게 처리할지에 대한 규칙을 정해주는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc17e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 실제 데이터 처리 작업을 수행하는 헬퍼(Helper) 함수 ---\n",
    "\n",
    "# PetID 하나를 입력받아 관련된 모든 추가 특징을 추출하는 함수\n",
    "def extract_additional_features(pet_id, mode='train'):\n",
    "    \n",
    "    # Sentiment 파일 경로를 생성\n",
    "    sentiment_filename = '../input/{}_sentiment/{}.json'.format(mode, pet_id)\n",
    "    try:\n",
    "        # 파일 열기\n",
    "        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n",
    "        # 내용 파싱하여 sentiment 데이터프레임 생성\n",
    "        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n",
    "        df_sentiment['PetID'] = pet_id\n",
    "    except FileNotFoundError: # 파일을 찾지 못하면\n",
    "        df_sentiment = [] # 빈 리스트로 처리\n",
    "\n",
    "    # Metadata 파일 처리를 위한 빈 리스트 준비\n",
    "    dfs_metadata = []\n",
    "    # 해당 PetID로 시작하는 모든 metadata 파일을 찾음 (예: a1b2c3-1.json, a1b2c3-2.json)\n",
    "    metadata_filenames = sorted(glob.glob('../input/{}_metadata/{}*.json'.format(mode, pet_id)))\n",
    "    \n",
    "    # 찾은 파일이 하나 이상 있다면\n",
    "    if len(metadata_filenames) > 0:\n",
    "        # 각 파일을 순회하며\n",
    "        for f in metadata_filenames:\n",
    "            # 파일 열기\n",
    "            metadata_file = pet_parser.open_metadata_file(f)\n",
    "            # 내용 파싱하여 metadata 데이터프레임 생성\n",
    "            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n",
    "            df_metadata['PetID'] = pet_id\n",
    "            # 처리된 결과를 dfs_metadata 리스트에 추가\n",
    "            dfs_metadata.append(df_metadata)\n",
    "        # 한 PetID에 여러 metadata 파일이 있을 경우, 이들을 하나의 데이터프레임으로 합침\n",
    "        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n",
    "    \n",
    "    # 최종적으로 sentiment 결과와 metadata 결과를 리스트에 담아 반환\n",
    "    dfs = [df_sentiment, dfs_metadata]\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6172c1",
   "metadata": {},
   "source": [
    "2. extract_additional_features 함수: 실제 행동대장\n",
    "이 함수는 위에서 만든 PetFinderParser라는 도구를 실제로 사용하는 역할을 합니다.\n",
    "\n",
    "PetID 하나를 건네주면, 해당 ID에 맞는 sentiment와 metadata 파일들을 찾아 pet_parser에게 넘겨 처리를 맡깁니다.\n",
    "\n",
    "try...except 구문을 사용해 파일이 없는 경우에도 오류로 멈추지 않고 유연하게 대처하며, 한 PetID에 여러 이미지(메타데이터 파일)가 있는 경우도 처리할 수 있도록 설계되었습니다.\n",
    "\n",
    "이 함수가 바로 Parallel을 통해 수천 번씩 호출될 실제 작업 단위입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95523c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 설계한 PetFinderParser 클래스의 인스턴스(실체)를 생성\n",
    "# 이제 'pet_parser'라는 변수를 통해 클래스 내의 모든 함수를 사용할 수 있음\n",
    "pet_parser = PetFinderParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8794c568",
   "metadata": {},
   "source": [
    "3. pet_parser = PetFinderParser(): 설계도를 실체로\n",
    "마지막 줄은 위에서 정의한 PetFinderParser 설계도를 바탕으로 pet_parser라는 실제 \"일꾼\" 객체를 만드는 과정입니다. 이제부터 pet_parser.open_metadata_file()과 같은 방식으로 클래스 안의 함수들을 호출할 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 처리할 PetID 목록 준비 ---\n",
    "# 디버깅 모드 플래그. False일 경우 전체 데이터를, True일 경우 일부 데이터만 사용.\n",
    "debug = False\n",
    "# train, test 데이터프레임에서 고유한 PetID만 추출하여 목록 생성\n",
    "train_pet_ids = train.PetID.unique()\n",
    "test_pet_ids = test.PetID.unique()\n",
    "\n",
    "# 디버그 모드가 True일 경우, 실행 시간을 줄이기 위해 일부 ID만 선택\n",
    "if debug:\n",
    "    train_pet_ids = train_pet_ids[:1000] # train ID 1000개\n",
    "    test_pet_ids = test_pet_ids[:500]   # test ID 500개\n",
    "\n",
    "\n",
    "# --- 2. Train 세트 처리 ---\n",
    "# Parallel을 이용한 병렬 처리 시작\n",
    "# n_jobs=6: 6개의 CPU 코어를 사용\n",
    "# verbose=1: 처리 진행 상황을 간단한 로그로 표시\n",
    "dfs_train = Parallel(n_jobs=6, verbose=1)(\n",
    "    # train_pet_ids의 모든 ID에 대해 extract_additional_features 함수를 실행\n",
    "    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids\n",
    ")\n",
    "\n",
    "# --- 3. Train 세트 결과 정리 ---\n",
    "# 병렬 처리 결과(dfs_train)에서 sentiment와 metadata 데이터프레임을 각각 분리\n",
    "# x[0]에 있는 sentiment 결과가 데이터프레임일 경우에만 리스트에 추가\n",
    "train_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\n",
    "# x[1]에 있는 metadata 결과가 데이터프레임일 경우에만 리스트에 추가\n",
    "train_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n",
    "\n",
    "# 분리된 데이터프레임 리스트를 하나의 큰 데이터프레임으로 결합(concat)\n",
    "train_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\n",
    "train_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n",
    "\n",
    "# 최종적으로 생성된 데이터프레임의 형태(shape)를 출력 (행 수, 열 수)\n",
    "print(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n",
    "\n",
    "\n",
    "# --- 4. Test 세트 처리 및 정리 ---\n",
    "# (위 Train 세트 처리 과정과 완전히 동일한 로직을 Test 데이터에 적용)\n",
    "dfs_test = Parallel(n_jobs=6, verbose=1)(\n",
    "    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids\n",
    ")\n",
    "\n",
    "test_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\n",
    "test_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n",
    "\n",
    "test_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\n",
    "test_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n",
    "\n",
    "print(test_dfs_sentiment.shape, test_dfs_metadata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4506d95",
   "metadata": {},
   "source": [
    "Parallel을 이용한 병렬 처리를 통해 처리 성능 향상   \n",
    "\n",
    "4개의 데이터셋(train_sentiment_df, train_metadata_df, test_sentiment_df, test_metadata_df) 생성, 이 4개의 df는 원본 train, test 데이터에 새로운 정보를 더해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea2ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞으로 사용할 집계(aggregation) 함수들을 리스트로 정의\n",
    "aggregates = ['mean', 'sum']\n",
    "\n",
    "\n",
    "# --- 1. Train 세트 - Metadata 처리 ---\n",
    "\n",
    "# -- 1a. 텍스트 데이터(annots_top_desc) 분리 및 처리 --\n",
    "# PetID를 기준으로 그룹화하고, 각 PetID의 모든 'metadata_annots_top_desc' 텍스트를 중복 없이 리스트로 묶음\n",
    "train_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\n",
    "train_metadata_desc = train_metadata_desc.reset_index() # 그룹화된 인덱스를 컬럼으로 변환\n",
    "# 텍스트 리스트를 공백으로 구분된 하나의 긴 문자열로 합침\n",
    "train_metadata_desc[\n",
    "    'metadata_annots_top_desc'] = train_metadata_desc[\n",
    "    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# -- 1b. 수치 데이터 처리 --\n",
    "prefix = 'metadata' # 컬럼명에 사용할 접두사\n",
    "# 원본에서 텍스트 컬럼을 제외하여 수치 데이터만 남김\n",
    "train_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\n",
    "# PetID를 제외한 모든 컬럼을 float(실수) 타입으로 변환 (집계를 위해)\n",
    "for i in train_metadata_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\n",
    "# PetID를 기준으로 그룹화하고, 각 수치 컬럼에 대해 평균(mean)과 합계(sum)를 계산\n",
    "train_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\n",
    "# 생성된 멀티인덱스 컬럼명을 'prefix_원본컬럼명_집계함수' 형식으로 재정의 (예: metadata_annots_score_MEAN)\n",
    "train_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in train_metadata_gr.columns.tolist()])\n",
    "train_metadata_gr = train_metadata_gr.reset_index() # 그룹화된 인덱스를 컬럼으로 변환\n",
    "\n",
    "\n",
    "# --- 2. Train 세트 - Sentiment 처리 ---\n",
    "# (위 Metadata 처리와 완전히 동일한 과정)\n",
    "\n",
    "train_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\n",
    "train_sentiment_desc = train_sentiment_desc.reset_index()\n",
    "train_sentiment_desc[\n",
    "    'sentiment_entities'] = train_sentiment_desc[\n",
    "    'sentiment_entities'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'sentiment'\n",
    "train_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\n",
    "for i in train_sentiment_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\n",
    "train_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(aggregates)\n",
    "train_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in train_sentiment_gr.columns.tolist()])\n",
    "train_sentiment_gr = train_sentiment_gr.reset_index()\n",
    "\n",
    "\n",
    "# --- 3. Test 세트 처리 ---\n",
    "# (위 Train 세트 처리와 완전히 동일한 과정을 Test 데이터에 적용)\n",
    "\n",
    "# Metadata 처리\n",
    "test_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\n",
    "# ... (이하 동일)\n",
    "# ...\n",
    "\n",
    "# Sentiment 처리\n",
    "test_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\n",
    "# ... (이하 동일)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa6a50",
   "metadata": {},
   "source": [
    "- 한 반려동물이 여러 개의 파일을 가질 수 있음 -> 하나의 PetID에 여러 행의 데이터가 생기기에, 원본 train 데이터와 합치기가 곤란해질 수 있음\n",
    "\n",
    "- 텍스트와 수치 데이터가 섞여있기 때문에 각각 다른 전략 사용\n",
    "  1. 텍스트 데이터 처리 (_desc 접미사)\n",
    "  여러 파일에서 나온 텍스트들을 .join() 함수를 사용해 하나의 긴 텍스트로 이어 붙임\n",
    "  2. 수치 데이터 처리 (_gr 접미사)\n",
    "  .agg(['mean', 'sum'])을 이용해 각 PetID별로 모든 수치 정보의 **평균(mean)과 합계(sum)**를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Train 세트 데이터 병합(Merge) ---\n",
    "\n",
    "# 원본 train 데이터프레임의 복사본을 만들어 작업 (원본 데이터 보존)\n",
    "train_proc = train.copy()\n",
    "# 'PetID'를 기준으로, train_proc에 수치형 sentiment 특징(_gr)을 'left' 방식으로 병합\n",
    "train_proc = train_proc.merge(\n",
    "    train_sentiment_gr, how='left', on='PetID')\n",
    "# 이어서 수치형 metadata 특징(_gr)을 병합\n",
    "train_proc = train_proc.merge(\n",
    "    train_metadata_gr, how='left', on='PetID')\n",
    "# 이어서 텍스트형 metadata 특징(_desc)을 병합\n",
    "train_proc = train_proc.merge(\n",
    "    train_metadata_desc, how='left', on='PetID')\n",
    "# 마지막으로 텍스트형 sentiment 특징(_desc)을 병합\n",
    "train_proc = train_proc.merge(\n",
    "    train_sentiment_desc, how='left', on='PetID')\n",
    "\n",
    "# --- 2. Test 세트 데이터 병합(Merge) ---\n",
    "\n",
    "# 원본 test 데이터프레임의 복사본을 만들어 작업\n",
    "test_proc = test.copy()\n",
    "# (위 Train 세트 병합과 동일한 과정을 Test 데이터에 적용)\n",
    "test_proc = test_proc.merge(\n",
    "    test_sentiment_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_metadata_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_metadata_desc, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_sentiment_desc, how='left', on='PetID')\n",
    "\n",
    "\n",
    "# --- 3. 최종 결과 확인 및 검증 ---\n",
    "\n",
    "# 모든 특징이 병합된 최종 데이터프레임의 형태(shape)를 출력\n",
    "print(train_proc.shape, test_proc.shape)\n",
    "# 병합 후 train 데이터의 행(row) 수가 원본과 동일한지 확인 (데이터 유실/중복 방지)\n",
    "assert train_proc.shape[0] == train.shape[0]\n",
    "# 병합 후 test 데이터의 행 수가 원본과 동일한지 확인\n",
    "assert test_proc.shape[0] == test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041651fe",
   "metadata": {},
   "source": [
    "- Data Merging 단계\n",
    "\n",
    "- **how='left'의 중요성**\n",
    "merge를 수행할 때 how='left' 옵션은 매우 중요하다. SQL의 left join과 의미는 같다.\n",
    "\n",
    "- **assert를 이용한 검증**\n",
    "assert 구문은 프로그래머의 \"안전장치\" 또는 \"자동 점검\" 기능이다. assert 뒤의 조건이 True가 아니면, AssertionError가 발생\n",
    "\n",
    "여기서 assert train_proc.shape[0] == train.shape[0]는 \"병합 후 데이터의 행 수가 원본과 정확히 일치해야만 한다\"는 것을 강력하게 확인하는 과정으로 만약 병합 과정에서 무언가 잘못되어 데이터 행이 중복되거나 사라졌다면, 이 assert 구문이 즉시 문제를 알려주어 버그를 조기에 발견할 수 있게 해줌\n",
    "\n",
    "이 단계를 거쳐 생성된 train_proc와 test_proc는 모든 특징이 결합된, 모델 학습을 위한 최종 준비를 마친 데이터셋입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03118bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Train 세트의 품종(Breed) 정보 변환 ---\n",
    "\n",
    "# -- 1a. 첫 번째 품종(Breed1) 처리 --\n",
    "# 'Breed1' 컬럼과 품종 이름이 담긴 'labels_breed'를 병합\n",
    "train_breed_main = train_proc[['Breed1']].merge(\n",
    "    labels_breed, how='left',         # 'Breed1'의 모든 값을 유지\n",
    "    left_on='Breed1', right_on='BreedID', # 'Breed1'과 'BreedID'를 기준으로 병합\n",
    "    suffixes=('', '_main_breed'))     # 중복 컬럼명에 접미사 추가\n",
    "\n",
    "# 병합 결과에서 불필요한 ID 컬럼들(Breed1, BreedID)을 제외하고, 실제 정보가 담긴 컬럼만 남김\n",
    "train_breed_main = train_breed_main.iloc[:, 2:]\n",
    "# 남은 컬럼들의 이름 앞에 'main_breed_' 접두사를 붙여 첫 번째 품종 정보임을 명시\n",
    "train_breed_main = train_breed_main.add_prefix('main_breed_')\n",
    "\n",
    "# -- 1b. 두 번째 품종(Breed2) 처리 (위와 동일한 과정 반복) --\n",
    "train_breed_second = train_proc[['Breed2']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed2', right_on='BreedID',\n",
    "    suffixes=('', '_second_breed'))\n",
    "\n",
    "train_breed_second = train_breed_second.iloc[:, 2:]\n",
    "train_breed_second = train_breed_second.add_prefix('second_breed_')\n",
    "\n",
    "\n",
    "# -- 1c. 변환된 품종 정보를 원본 데이터에 최종 결합 --\n",
    "# 기존 train_proc 데이터프레임 옆에(axis=1) main_breed와 second_breed 정보를 붙임\n",
    "train_proc = pd.concat(\n",
    "    [train_proc, train_breed_main, train_breed_second], axis=1)\n",
    "\n",
    "\n",
    "# --- 2. Test 세트의 품종(Breed) 정보 변환 ---\n",
    "# (위 Train 세트 처리와 완전히 동일한 과정을 Test 데이터에 적용)\n",
    "\n",
    "test_breed_main = test_proc[['Breed1']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed1', right_on='BreedID',\n",
    "    suffixes=('', '_main_breed'))\n",
    "# ... (이하 동일)\n",
    "\n",
    "\n",
    "# --- 3. 최종 결과 확인 ---\n",
    "# 품종 정보가 추가된 최종 데이터프레임의 형태(shape)를 출력\n",
    "print(train_proc.shape, test_proc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f0a46",
   "metadata": {},
   "source": [
    "- 부가 설명\n",
    "이 코드 블록의 핵심 목표는 의미 없는 숫자 ID를 해석 가능한 텍스트 정보로 변환하는 특징 생성(Feature Creation) 과정입니다.\n",
    "\n",
    "train 데이터에 있는 Breed1, Breed2 컬럼은 '307'이나 '266' 같은 숫자로만 이루어져 있습니다. 이 숫자 자체는 모델에게 아무런 의미를 주지 못합니다. 이 코드는 labels_breed.csv라는 **참조표(Lookup Table)**를 이용해 이 숫자 ID들을 'Mixed Breed', 'Domestic Short Hair' 같은 실제 품종 이름으로 바꾸어주는 역할을 합니다.\n",
    "\n",
    "- 처리 과정 요약\n",
    "정보 매칭 (merge): train_proc의 Breed1 ID를 labels_breed의 BreedID와 비교하여 일치하는 품종 이름(BreedName)과 타입(Type) 정보를 찾아 옆에 붙입니다.\n",
    "\n",
    "정리 및 이름 부여 (iloc, add_prefix):\n",
    "\n",
    "병합에 사용된 불필요한 ID 컬럼들을 iloc으로 제거하여 순수한 품종 정보만 남깁니다.\n",
    "\n",
    "첫 번째 품종(Breed1)에서 온 정보인지, 두 번째 품종(Breed2)에서 온 정보인지 명확히 구분하기 위해, 각 컬럼명 앞에 main_breed_, second_breed_ 와 같은 접두사를 붙여줍니다. 이는 나중에 모델 분석 시 혼동을 막아줍니다.\n",
    "\n",
    "최종 결합 (concat):\n",
    "\n",
    "이렇게 깔끔하게 정리된 품종 이름 정보(train_breed_main, train_breed_second)를 원래의 train_proc 데이터프레임의 오른쪽에 열(column) 방향으로 이어 붙입니다 (axis=1).\n",
    "\n",
    "이 과정을 통해 기존의 숫자 ID 컬럼 외에, 모델이 더 잘 이해하고 활용할 수 있는 새로운 텍스트 기반의 특징(feature) 컬럼들이 데이터셋에 추가됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)\n",
    "print('NaN structure:\\n{}'.format(np.sum(pd.isnull(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba653029",
   "metadata": {},
   "source": [
    "- 본격적인 모델링에 앞서 데이터를 최종적으로 통합하고 마지막으로 점검\n",
    "\n",
    "- 왜 합쳤는가?   \n",
    "\n",
    "  - 머신러닝 모델을 만들기 전, 데이터를 전처리(예: 텍스트 인코딩, 숫자 스케일링 등)해야 합니다. 이때 train 데이터와 test 데이터는 반드시 동일한 기준과 방법으로 처리되어야 합니다.\n",
    "\n",
    "  - 만약 두 데이터를 따로따로 전처리하면, 기준이 미세하게 달라져 모델 성능에 문제가 생길 수 있습니다.\n",
    "\n",
    "  - 그래서 가장 효율적이고 안전한 방법은 다음과 같습니다.\n",
    "    1. 일단 합친다 (pd.concat): train과 test를 하나의 큰 데이터(X)로 합칩니다.\n",
    "    2. 한 번에 처리한다: 합쳐진 데이터 X에 모든 전처리 과정을 한꺼번에 적용합니다.\n",
    "    3. 다시 나눈다: 전처리가 끝나면, AdoptionSpeed 열에 값이 있는지 없는지를 기준으로 다시 X_train과 X_test로 분리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51823df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 데이터 타입별로 컬럼 분류 ---\n",
    "# 통합 데이터프레임 X의 각 컬럼별 데이터 타입(dtype)을 확인\n",
    "column_types = X.dtypes\n",
    "\n",
    "# 데이터 타입이 정수(int)인 컬럼들만 선택\n",
    "int_cols = column_types[column_types == 'int']\n",
    "# 데이터 타입이 실수(float)인 컬럼들만 선택\n",
    "float_cols = column_types[column_types == 'float']\n",
    "# 데이터 타입이 객체(object)인 컬럼들만 선택 (주로 문자열 데이터)\n",
    "cat_cols = column_types[column_types == 'object']\n",
    "\n",
    "# --- 2. 분류 결과 출력 ---\n",
    "# 분류된 정수형 컬럼들의 목록을 출력\n",
    "print('\\tinteger columns:\\n{}'.format(int_cols))\n",
    "# 분류된 실수형 컬럼들의 목록을 출력\n",
    "print('\\n\\tfloat columns:\\n{}'.format(float_cols))\n",
    "# 분류된 범주형(object) 컬럼들의 목록을 출력. 이 컬럼들은 인코딩이 필요함.\n",
    "print('\\n\\tto encode categorical columns:\\n{}'.format(cat_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411fc38",
   "metadata": {},
   "source": [
    "- 각 컬럼들의 데이터 타입을 구분해두면 후처리가 쉬워진답니다~ (수치형 컬럼들에는 스케일링을, 범주형 컬럼들에는 인코딩을 적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original X DF for easier experimentation,\n",
    "# all feature engineering will be performed on this one:\n",
    "X_temp = X.copy()\n",
    "\n",
    "\n",
    "# Select subsets of columns:\n",
    "text_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\n",
    "categorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n",
    "\n",
    "# Names are all unique, so they can be dropped by default\n",
    "# Same goes for PetID, it shouldn't be used as a feature\n",
    "to_drop_columns = ['PetID', 'Name', 'RescuerID']\n",
    "# RescuerID will also be dropped, as a feature based on this column will be extracted independently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ae482",
   "metadata": {},
   "source": [
    "- text_columns: 자연어 처리(NLP) 기술(예: TF-IDF)을 적용해야 하는 컬럼들입니다.\n",
    "\n",
    "- categorical_columns: 품종 이름처럼 정해진 몇 개의 카테고리로 이루어진 컬럼들입니다. 이들은 주로 원-핫 인코딩(One-Hot Encoding)으로 처리됩니다.\n",
    "\n",
    "- to_drop_columns: 모델의 예측 성능에 도움이 되지 않거나, 오히려 방해가 될 수 있는 컬럼들을 미리 지정합니다. PetID나 Name처럼 모든 행마다 값이 다른 고유 식별자는 모델이 암기하게 만들어 과적합(overfitting)을 유발할 수 있으므로 제거하는 것이 일반적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count RescuerID occurrences:\n",
    "rescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\n",
    "rescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n",
    "\n",
    "# Merge as another feature onto main DF:\n",
    "X_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a89be1",
   "metadata": {},
   "source": [
    "- RescuerID_COUNT 라는 파생 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize categorical columns:\n",
    "for i in categorical_columns:\n",
    "    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51fcb25",
   "metadata": {},
   "source": [
    "- 라벨인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cff653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset text features:\n",
    "X_text = X_temp[text_columns]\n",
    "\n",
    "for i in X_text.columns:\n",
    "    X_text.loc[:, i] = X_text.loc[:, i].fillna('<MISSING>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf91604",
   "metadata": {},
   "source": [
    "이 코드 블록은 **자연어 처리(NLP)**를 위한 텍스트 데이터를 따로 준비하고, 처리 과정에서 오류를 방지하기 위해 결측치를 관리하는 단계입니다.\n",
    "\n",
    "텍스트 데이터 분리: 전체 데이터(X_temp)에서 텍스트 분석이 필요한 컬럼들만 골라 X_text라는 별도의 데이터프레임으로 만드는 것은 효율적인 작업 방식입니다. 이렇게 하면 숫자나 다른 종류의 데이터에 영향을 주지 않고 텍스트 관련 전처리(예: TF-IDF)를 깔끔하게 적용할 수 있습니다.\n",
    "\n",
    "결측치 처리의 중요성: 텍스트 데이터를 벡터화하는 라이브러리(예: TfidfVectorizer)는 NaN과 같은 비어있는 값을 만나면 오류를 일으킵니다. 따라서 모든 값을 문자열 형태로 만들어주어야 합니다. .fillna('<MISSING>')은 이러한 NaN 값들을 '<MISSING>'이라는 일관된 문자열로 대체하여 후속 작업이 원활하게 진행되도록 보장합니다.\n",
    "\n",
    "또한, 단순히 빈칸('')으로 채우는 대신 '<MISSING>'이라는 의미 있는 문자열로 채우면, \"설명이 없는 경우\" 자체가 하나의 중요한 특징(feature)으로 작용하여 모델이 학습하는 데 도움이 될 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14056b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 텍스트 특징 추출을 위한 라이브러리 임포트 ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF # 차원 축소 기법\n",
    "\n",
    "# --- 2. 파라미터 및 변수 초기화 ---\n",
    "n_components = 5      # 각 텍스트 컬럼에서 추출할 주요 토픽(topic) 또는 특징(feature)의 개수\n",
    "text_features = []    # 생성된 텍스트 특징들을 임시로 저장할 리스트\n",
    "\n",
    "# --- 3. 각 텍스트 컬럼별 특징 생성 ---\n",
    "# 이전에 준비한 텍스트 컬럼들('Description' 등)에 대해 반복\n",
    "for i in X_text.columns:\n",
    "    \n",
    "    # -- 3a. 알고리즘 초기화 --\n",
    "    print('generating features from: {}'.format(i))\n",
    "    # Truncated SVD 모델 초기화 (n_components개의 특징 추출)\n",
    "    svd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n",
    "    # NMF 모델 초기화 (n_components개의 특징 추출)\n",
    "    nmf_ = NMF(n_components=n_components, random_state=1337)\n",
    "    \n",
    "    # -- 3b. TF-IDF 벡터화 --\n",
    "    # TfidfVectorizer를 이용해 텍스트 데이터를 수치 벡터(TF-IDF 행렬)로 변환\n",
    "    tfidf_col = TfidfVectorizer().fit_transform(X_text.loc[:, i].values)\n",
    "    \n",
    "    # -- 3c. SVD 적용 및 데이터프레임 변환 --\n",
    "    # TF-IDF 행렬에 SVD를 적용하여 n_components개의 주요 특징으로 압축\n",
    "    svd_col = svd_.fit_transform(tfidf_col)\n",
    "    svd_col = pd.DataFrame(svd_col) # 결과를 데이터프레임으로 변환\n",
    "    svd_col = svd_col.add_prefix('SVD_{}_'.format(i)) # 컬럼명에 접두사를 붙여 구분\n",
    "    \n",
    "    # -- 3d. NMF 적용 및 데이터프레임 변환 --\n",
    "    # TF-IDF 행렬에 NMF를 적용하여 n_components개의 주요 특징으로 압축\n",
    "    nmf_col = nmf_.fit_transform(tfidf_col)\n",
    "    nmf_col = pd.DataFrame(nmf_col) # 결과를 데이터프레임으로 변환\n",
    "    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i)) # 컬럼명에 접두사를 붙여 구분\n",
    "    \n",
    "    # -- 3e. 결과 저장 --\n",
    "    # 생성된 SVD와 NMF 특징을 text_features 리스트에 추가\n",
    "    text_features.append(svd_col)\n",
    "    text_features.append(nmf_col)\n",
    "\n",
    "    \n",
    "# --- 4. 모든 텍스트 특징 결합 ---\n",
    "# text_features 리스트에 저장된 모든 데이터프레임 조각들을 옆으로 이어 붙임 (axis=1)\n",
    "text_features = pd.concat(text_features, axis=1)\n",
    "\n",
    "# --- 5. 최종 데이터프레임에 병합 및 원본 텍스트 제거 ---\n",
    "# 생성된 텍스트 특징들을 메인 데이터프레임(X_temp)에 결합\n",
    "X_temp = pd.concat([X_temp, text_features], axis=1)\n",
    "\n",
    "# 원본 텍스트 컬럼들은 이제 불필요하므로 제거\n",
    "for i in X_text.columns:\n",
    "    X_temp = X_temp.drop(i, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f327c274",
   "metadata": {},
   "source": [
    "이 코드 블록은 **자연어 처리(NLP)**의 핵심적인 부분으로, 컴퓨터가 이해할 수 없는 텍스트 데이터를 의미 있는 숫자형 특징(feature)으로 변환하는 과정입니다. 이 과정은 크게 두 단계로 나뉩니다.\n",
    "\n",
    "1. 텍스트의 수치화: TfidfVectorizer\n",
    "컴퓨터는 '고양이'라는 단어 자체를 이해하지 못합니다. **TF-IDF(Term Frequency-Inverse Document Frequency)**는 각 텍스트(문서)에서 단어의 중요도를 계산하여, 텍스트 데이터를 거대한 숫자 행렬로 변환하는 기법입니다.\n",
    "\n",
    "TF (Term Frequency): 한 문서 안에서 특정 단어가 얼마나 자주 등장하는지.\n",
    "\n",
    "IDF (Inverse Document Frequency): 특정 단어가 전체 문서들 중에서 얼마나 희귀하게 등장하는지. (모든 문서에 다 나오는 단어는 중요도가 낮음)\n",
    "\n",
    "이 과정을 거치면 각 텍스트는 수만 개의 단어(컬럼)로 이루어진 매우 큰 숫자 벡터가 됩니다.\n",
    "\n",
    "2. 정보 압축 및 토픽 추출: TruncatedSVD & NMF\n",
    "TF-IDF로 변환된 데이터는 너무 크고(수만 개의 컬럼) 정보가 흩어져 있어 모델이 학습하기 어렵습니다. 차원 축소(Dimensionality Reduction) 기법은 이 방대한 정보를 몇 개의 핵심적인 '토픽(Topic)' 또는 **'잠재 의미(Latent Semantic)'**로 압축하는 역할을 합니다.\n",
    "\n",
    "TruncatedSVD (특이값 분해): 텍스트에 숨어있는 주요 패턴과 의미를 찾아내어, n_components개(여기서는 5개)의 가장 중요한 특징으로 정보를 요약합니다.\n",
    "\n",
    "NMF (음수 미포함 행렬 분해): SVD와 유사하게 토픽을 추출하지만, 모든 값이 양수여야 한다는 제약 조건이 있어 좀 더 해석이 쉬운 경향이 있습니다.\n",
    "\n",
    "결과적으로, 하나의 긴 반려동물 설명문은 \"활발함에 대한 점수\", \"친근함에 대한 점수\" 등과 같이 5개의 주요 토픽 점수로 압축됩니다. 이 코드에서는 SVD와 NMF 두 가지 기법을 모두 사용하여 다양한 관점의 특징을 추출하고 있습니다.\n",
    "\n",
    "이 과정을 거쳐 원본 텍스트 컬럼들은 모델이 학습할 수 있는 10개(= 5개(SVD) + 5개(NMF))의 새로운 숫자형 특징 컬럼으로 대체됩니다.\n",
    "\n",
    "- \"의미를 알 수 없는 긴 텍스트\" → TF-IDF → \"수만 개의 단어 점수 벡터\" → SVD/NMF → \"모델이 학습할 수 있는 5개의 핵심 토픽 점수\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns:\n",
    "X_temp = X_temp.drop(to_drop_columns, axis=1)\n",
    "\n",
    "# Check final df shape:\n",
    "print('X shape: {}'.format(X_temp.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bbf5b4",
   "metadata": {},
   "source": [
    "- 불필요한 컬럼 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e92560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 데이터를 다시 Train과 Test로 분리 ---\n",
    "\n",
    "# 'AdoptionSpeed' 컬럼에 유한한 값(숫자)이 있는 행들을 X_train으로 선택 (원본 train 데이터)\n",
    "X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\n",
    "# 'AdoptionSpeed' 컬럼에 유한한 값이 없는 행(NaN)들을 X_test로 선택 (원본 test 데이터)\n",
    "X_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n",
    "\n",
    "# --- 2. Test 세트에서 정답 컬럼 제거 ---\n",
    "\n",
    "# X_test 데이터프레임에는 정답이 없어야 하므로, 'AdoptionSpeed' 컬럼을 제거\n",
    "X_test = X_test.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "\n",
    "# --- 3. 분리 결과 확인 및 검증 ---\n",
    "\n",
    "# 최종적으로 분리된 X_train과 X_test의 형태(shape)를 출력\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('X_test shape: {}'.format(X_test.shape))\n",
    "\n",
    "# 분리된 X_train의 행 수가 원본 train의 행 수와 동일한지 확인\n",
    "assert X_train.shape[0] == train.shape[0]\n",
    "# 분리된 X_test의 행 수가 원본 test의 행 수와 동일한지 확인\n",
    "assert X_test.shape[0] == test.shape[0]\n",
    "\n",
    "\n",
    "# --- 4. 두 데이터프레임의 컬럼 일치 여부 확인 ---\n",
    "\n",
    "# X_train의 컬럼 목록에서 정답('AdoptionSpeed') 컬럼을 제외\n",
    "train_cols = X_train.columns.tolist()\n",
    "train_cols.remove('AdoptionSpeed')\n",
    "\n",
    "# X_test의 컬럼 목록을 가져옴\n",
    "test_cols = X_test.columns.tolist()\n",
    "\n",
    "# 두 컬럼 목록이 순서까지 완벽하게 동일한지 확인\n",
    "assert np.all(train_cols == test_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341bfd80",
   "metadata": {},
   "source": [
    "- 모델 학습을 위해 다시 train과 test 세트로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pd.isnull(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0f4b6",
   "metadata": {},
   "source": [
    "- train 결측치 개수 최종 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72094cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pd.isnull(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f9099",
   "metadata": {},
   "source": [
    "- test 결측치 개수 최종 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d25f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 라이브러리 임포트 ---\n",
    "import scipy as sp  # 과학 계산, 특히 최적화(optimize)를 위해 사용\n",
    "\n",
    "from collections import Counter # 데이터의 요소 개수를 세는 기능\n",
    "from functools import partial   # 함수의 인자 중 일부를 고정하여 새로운 함수를 만드는 기능\n",
    "from math import sqrt           # 제곱근 계산\n",
    "\n",
    "# Scikit-learn에서 평가 지표(metrics) 관련 함수들을 임포트\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "\n",
    "\n",
    "# --- 2. 평가 지표(Quadratic Weighted Kappa) 계산 함수 ---\n",
    "# 원본 출처: https://github.com/benhamner/Metrics\n",
    "\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    두 평가자(실제값, 예측값) 간의 혼동 행렬(Confusion Matrix)을 생성합니다.\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b)) # 두 평가값의 개수가 동일한지 확인\n",
    "    # 최소/최대 평점(rating)이 주어지지 않으면 데이터에서 자동으로 찾음\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    # 평점의 종류 개수 계산 (0, 1, 2, 3, 4 -> 5개)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    # N x N 크기의 0으로 채워진 행렬 생성\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    # 실제값(a)과 예측값(b)을 하나씩 비교하며 해당 위치의 카운트를 1씩 증가\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    주어진 평점(rating) 데이터의 분포(히스토그램)를 계산합니다.\n",
    "    \"\"\"\n",
    "    # ... (confusion_matrix와 유사한 로직으로 각 평점이 몇 번 나왔는지 셈)\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    이 대회의 공식 평가 지표인 Quadratic Weighted Kappa 점수를 계산합니다.\n",
    "    실제값(y)과 예측값(y_pred)이 얼마나 일치하는지를 측정합니다. (-1 ~ 1 사이의 값)\n",
    "    단순히 맞고 틀림을 넘어, '얼마나 심하게' 틀렸는지에 가중치를 두어 점수를 매깁니다.\n",
    "    \"\"\"\n",
    "    # ... (내부적으로 위의 confusion_matrix, histogram 함수를 사용하여 복잡한 가중치 계산 수행)\n",
    "    # ...\n",
    "    return (1.0 - numerator / denominator)\n",
    "\n",
    "# --- 3. 최적의 분류 경계값을 찾는 클래스 ---\n",
    "class OptimizedRounder(object):\n",
    "    \"\"\"\n",
    "    회귀 모델이 예측한 연속적인 값(예: 2.3, 1.8)을\n",
    "    어떤 경계값(threshold)으로 잘라야 가장 높은 Kappa 점수를 얻을 수 있는지\n",
    "    그 최적의 경계값을 찾아주는 클래스입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0 # 최적의 경계값(계수)을 저장할 변수\n",
    "\n",
    "    # Kappa 점수를 '손실 함수'로 변환하는 내부 함수 (최적화 라이브러리는 손실을 최소화하므로)\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        # 주어진 경계값(coef)을 기준으로 예측값(X_p)을 0, 1, 2, 3, 4의 범주로 변환\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            # ... (이하 생략) ...\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        # 변환된 예측값으로 Kappa 점수를 계산\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        # Kappa 점수는 높을수록 좋으므로, 앞에 -를 붙여 '손실'로 변환 (최소화 문제로 만들기 위함)\n",
    "        return -ll\n",
    "\n",
    "    # 최적의 경계값을 찾는 메인 함수\n",
    "    def fit(self, X, y):\n",
    "        # 최적화할 손실 함수를 준비 (X와 y 값을 고정)\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        # 초기 경계값 설정\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        # scipy의 최적화 함수(minimize)를 사용해 손실(-kappa)을 최소화하는 경계값을 찾음\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    # 찾아낸 최적의 경계값으로 새로운 데이터를 예측하는 함수\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        # ... (_kappa_loss와 동일한 로직으로 예측) ...\n",
    "        return X_p\n",
    "\n",
    "    # 최적화 결과로 찾아낸 경계값(계수)을 반환하는 함수\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "\n",
    "# --- 4. RMSE 계산 함수 ---\n",
    "def rmse(actual, predicted):\n",
    "    \"\"\"\n",
    "    평균 제곱근 오차(Root Mean Squared Error)를 계산하는 간단한 함수.\n",
    "    \"\"\"\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b99dba",
   "metadata": {},
   "source": [
    "- 대회의 공식 평가 지표인 Quadratic Weighted Kappa 지표의 점수가 잘 나올수 있도록 계산 로직을 미리 설정\n",
    "\n",
    "-  OptimizedRounder: 회귀 모델을 위한 '필살기'\n",
    "\n",
    "많은 경우, 입양 속도(0, 1, 2, 3, 4)를 직접 예측하는 분류(Classification) 문제보다, 연속적인 값(예: 2.7, 3.1)을 예측하는 회귀(Regression) 문제로 접근하는 것이 성능이 더 좋을 때가 많습니다.\n",
    "\n",
    "하지만 회귀 모델의 예측값(2.7, 3.1)은 최종적으로 0, 1, 2, 3, 4 중 하나로 변환해야 합니다. 이때 단순히 반올림(예: 2.7 -> 3)하는 것보다 더 좋은 방법이 있습니다.\n",
    "\n",
    "OptimizedRounder 클래스는 바로 이 **최적의 변환 경계(Threshold)**를 찾아주는 역할을 합니다.\n",
    "\n",
    "작동 원리: scipy.optimize.minimize라는 최적화 도구를 사용하여, 어떤 경계값(예: [1.7, 2.5, 3.2, 3.8])으로 예측값을 잘라야 Kappa 점수가 가장 높아지는지를 자동으로 탐색합니다.\n",
    "\n",
    "효과: 단순히 반올림하는 것보다 훨씬 더 높은 최종 점수를 얻게 해주는 매우 중요한 후처리(Post-processing) 기법입니다.\n",
    "\n",
    "- 코드 블록 전체는 모델링의 핵심 로직이라기보다는, 모델의 성능을 정확히 측정하고 그 성능을 극한까지 끌어올리기 위한 고급 유틸리티 및 전략이라고 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.85,\n",
    "          'feature_fraction': 0.8,\n",
    "          'min_split_gain': 0.02,\n",
    "          'min_child_samples': 150,\n",
    "          'min_child_weight': 0.02,\n",
    "          'lambda_l2': 0.0475,\n",
    "          'verbosity': -1,\n",
    "          'data_random_seed': 17}\n",
    "\n",
    "# Additional parameters:\n",
    "early_stop = 500\n",
    "verbose_eval = 100\n",
    "num_rounds = 10000\n",
    "n_splits = 5\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_splits, random_state=1337)\n",
    "\n",
    "\n",
    "oof_train = np.zeros((X_train.shape[0]))\n",
    "oof_test = np.zeros((X_test.shape[0], n_splits))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for train_index, valid_index in kfold.split(X_train, X_train['AdoptionSpeed'].values):\n",
    "    \n",
    "    X_tr = X_train.iloc[train_index, :]\n",
    "    X_val = X_train.iloc[valid_index, :]\n",
    "    \n",
    "    y_tr = X_tr['AdoptionSpeed'].values\n",
    "    X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n",
    "    \n",
    "    y_val = X_val['AdoptionSpeed'].values\n",
    "    X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n",
    "    \n",
    "    print('\\ny_tr distribution: {}'.format(Counter(y_tr)))\n",
    "    \n",
    "    d_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    d_valid = lgb.Dataset(X_val, label=y_val)\n",
    "    watchlist = [d_train, d_valid]\n",
    "    \n",
    "    print('training LGB:')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    \n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    oof_train[valid_index] = val_pred\n",
    "    oof_test[:, i] = test_pred\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf92ce68",
   "metadata": {},
   "source": [
    "oof_train, oof_test의 역할\n",
    "oof_train (Out-of-Fold train): 각 데이터가 검증용으로 사용되었을 때의 예측값을 모아놓은 것입니다. 이는 모델이 한 번도 보지 못한 데이터에 대한 예측이므로, 이 oof_train과 실제 정답을 비교하면 모델의 일반화 성능을 가장 객관적으로 측정할 수 있습니다.\n",
    "\n",
    "oof_test: test 데이터에 대한 5개 모델의 예측값을 모두 저장합니다. 최종 제출 파일을 만들 때 이 값들의 평균을 사용하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(oof_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f5df3",
   "metadata": {},
   "source": [
    "1. 예측값의 중심 경향: 예측값들이 특정 값(예: 2.5) 주변에 많이 몰려 있는지, 아니면 넓게 퍼져 있는지를 시각적으로 파악할 수 있습니다.\n",
    "\n",
    "2. 분포의 형태: 분포가 정규분포와 비슷한지, 아니면 한쪽으로 치우쳐져 있는지 등을 확인할 수 있습니다.\n",
    "\n",
    "3. 경계값 설정의 단서: 이 히스토그램을 보고, 다음 단계인 OptimizedRounder를 사용할 때 최적의 분류 경계값이 대략 어디쯤 위치할지 직관적인 힌트를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 최적의 분류 경계값(Threshold) 찾기 ---\n",
    "# 이전에 정의한 OptimizedRounder 클래스의 인스턴스를 생성\n",
    "optR = OptimizedRounder()\n",
    "# OOF 예측값(oof_train)과 실제 정답(AdoptionSpeed)을 사용해 Kappa 점수를 최대화하는 경계값을 찾음\n",
    "optR.fit(oof_train, X_train['AdoptionSpeed'].values)\n",
    "# 위에서 찾은 최적의 경계값을 'coefficients' 변수에 저장\n",
    "coefficients = optR.coefficients()\n",
    "\n",
    "# --- 2. 최적 경계값을 적용하여 최종 예측 생성 ---\n",
    "# 찾은 경계값(coefficients)을 oof_train 예측값에 적용하여 최종 분류(0~4) 결과를 생성\n",
    "pred_test_y_k = optR.predict(oof_train, coefficients)\n",
    "\n",
    "# --- 3. 결과 확인 및 최종 점수 계산 ---\n",
    "# 실제 정답값들의 분포를 출력\n",
    "print(\"\\nValid Counts = \", Counter(X_train['AdoptionSpeed'].values))\n",
    "# 최적화된 예측값들의 분포를 출력\n",
    "print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "# 찾아낸 최적의 경계값들을 출력\n",
    "print(\"Coefficients = \", coefficients)\n",
    "# 최종 예측 결과와 실제 정답 사이의 Quadratic Weighted Kappa 점수를 계산\n",
    "qwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, pred_test_y_k)\n",
    "# 최종 QWK 점수를 출력\n",
    "print(\"QWK = \", qwk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34dfe2b",
   "metadata": {},
   "source": [
    "단순 반올림 대신 데이터에 최적화된 기준을 찾아 회귀 예측을 분류로 변환함으로써, 모델의 잠재 성능을 최대한으로 끌어내는 매우 중요한 후처리 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually adjusted coefficients:\n",
    "\n",
    "coefficients_ = coefficients.copy()\n",
    "\n",
    "coefficients_[0] = 1.645 # 첫 번째 경계선(0과 1을 나누는 선)의 위치를 1.645로 옮긴다.\n",
    "coefficients_[1] = 2.115 # 두 번째 경계선(1과 2를 나누는 선)의 위치를 2.115로 옮긴다.\n",
    "coefficients_[3] = 2.84 # 세 번째 경계선(3과 4를 나누는 선)의 위치를 2.84로 옮긴다.\n",
    "\n",
    "train_predictions = optR.predict(oof_train, coefficients_).astype(int)\n",
    "print('train pred distribution: {}'.format(Counter(train_predictions)))\n",
    "\n",
    "test_predictions = optR.predict(oof_test.mean(axis=1), coefficients_)\n",
    "print('test pred distribution: {}'.format(Counter(test_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c949fa4c",
   "metadata": {},
   "source": [
    "```\n",
    "<-- 등급 0 --|-- 등급 1 --|-- 등급 2 --|-- 등급 3 --|-- 등급 4 -->\n",
    "             ↑           ↑            ↑            ↑\n",
    "        경계선 1      경계선 2     경계선 3      경계선 4\n",
    "   (coefficients[0]) (coefficients[1]) (coefficients[2]) (coefficients[3])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00d20d",
   "metadata": {},
   "source": [
    "- 임의로 최적값 수정\n",
    "\n",
    "- 국소 최적해 (Local Minimum): 최적화 알고리즘이 완벽한 전역 최적해(Global Minimum)가 아닌, 그럴듯한 국소 최적해에 머물렀을 수 있습니다.\n",
    "\n",
    "- 데이터의 미묘한 특성: OOF 예측값의 분포나 실제 정답의 분포를 시각화해 본 분석가가 \"경계값을 약간만 옮기면 더 안정적인 예측이 가능하겠다\"고 판단할 수 있습니다. 예를 들어, 특정 경계값 근처에 예측값들이 너무 빽빽하게 모여 있다면, 약간의 조정으로 분류 결과가 더 안정될 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd3e64",
   "metadata": {},
   "source": [
    "국소 최적해는 주로 다음과 같은 경우에 발생합니다.\n",
    "\n",
    "1. 문제 자체가 복잡하고 울퉁불퉁할 때 (Non-convex Problems)\n",
    "가장 근본적인 원인입니다. 해결하려는 문제의 정답을 찾는 과정이 평탄한 언덕이 아니라, 여러 개의 봉우리와 계곡이 있는 험준한 산맥을 탐험하는 것과 같은 경우입니다.\n",
    "\n",
    "비유: 가장 높은 산봉우리(전체 최적해)를 찾는 것이 목표인데, 안개가 자욱해서 주변만 보입니다. 일단 가장 높은 곳으로 계속 올라가다 보니 어떤 봉우리(국소 최적해)에 도착했습니다. 하지만 안개 때문에 저 멀리 더 높은 진짜 정상(전체 최적해)이 있다는 것을 알지 못하고 탐험을 멈추게 됩니다.\n",
    "\n",
    "실제 예시: 딥러닝 모델의 학습 과정, 복잡한 시스템의 최적화 문제 등 수많은 변수들이 서로 얽혀있는 문제들이 여기에 해당합니다.\n",
    "\n",
    "2. 알고리즘이 '탐욕적(Greedy)'일 때\n",
    "알고리즘이 전체적인 큰 그림을 보지 않고, 매 순간 눈앞의 이익이 가장 큰 방향으로만 움직이는 방식일 때 국소 최적해에 빠지기 쉽습니다.\n",
    "\n",
    "비유: 동네에서 가장 높은 곳으로 가려고 합니다. 다른 길은 보지 않고, 무조건 현재 위치에서 가장 경사가 가파른 오르막길로만 계속 올라갑니다. 그러다 결국 동네 뒷산 정상에는 도착했지만, 도시 전체에서 가장 높은 남산타워로 가는 길은 놓치게 됩니다.\n",
    "\n",
    "실제 예시: 포트폴리오 예시처럼 \"일단 베타 낮은 것만 고르고, 그 안에서 수익률 제일 높은 것\"과 같이 단계별로 최선을 선택하는 방식, 일부 클러스터링 알고리즘 등이 여기에 해당합니다.\n",
    "\n",
    "3. 시작 지점이 좋지 않을 때\n",
    "최적화 알고리즘은 보통 임의의 지점에서 탐색을 시작합니다. 이때 어디서 시작했는지에 따라 최종적으로 도달하는 봉우리가 달라질 수 있습니다.\n",
    "\n",
    "비유: 헬리콥터가 두 명의 탐험가를 산맥의 서로 다른 지점에 내려주었습니다. 두 사람 모두 각자의 위치에서 가장 높은 봉우리를 향해 올라갔지만, 시작 위치가 달랐기 때문에 결국 서로 다른 봉우리에 도달하게 됩니다.\n",
    "\n",
    "실제 예시: K-평균 클러스터링(K-Means Clustering)은 처음에 중심점을 어디에 찍느냐에 따라 최종 클러스터 결과가 크게 달라집니다. 이를 방지하기 위해 여러 번 다른 시작점에서 실행해보는 방법을 사용합니다.\n",
    "\n",
    "4. 규칙(모델)이 너무 단순할 때 (사용자께서 지적하신 경우)\n",
    "문제의 복잡성을 충분히 담아내지 못하는 지나치게 단순한 모델이나 규칙을 사용하면, 다양한 가능성을 탐색하지 못하고 뻔한 결론에 도달하게 됩니다.\n",
    "\n",
    "비유: \"가장 돈을 많이 버는 방법은?\"이라는 질문에 \"가장 월급을 많이 주는 회사 한 군데에 취직하는 것\"이라고 답하는 것과 같습니다. 사업, 투자, 부업 등 다양한 조합을 통한 최적의 부 창출 방법을 고려하지 못한 것입니다.\n",
    "\n",
    "실제 예시: 사용자께서 경험하신 포트폴리오 '몰빵' 추천이 바로 이 경우에 해당합니다. 여러 자산을 조합했을 때의 위험 분산 효과라는 중요한 규칙을 모델이 고려하지 않은 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution inspection of original target and predicted train and test:\n",
    "\n",
    "print(\"True Distribution:\")\n",
    "print(pd.value_counts(X_train['AdoptionSpeed'], normalize=True).sort_index())\n",
    "print(\"\\nTrain Predicted Distribution:\")\n",
    "print(pd.value_counts(train_predictions, normalize=True).sort_index())\n",
    "print(\"\\nTest Predicted Distribution:\")\n",
    "print(pd.value_counts(test_predictions, normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a84c19b",
   "metadata": {},
   "source": [
    "- 모델이 예측한 결과와 실제 데이터의 분포 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113381b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission:\n",
    "\n",
    "submission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions.astype(np.int32)})\n",
    "submission.head()\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
